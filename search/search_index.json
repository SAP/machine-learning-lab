{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Machine Learning Lab \u00b6 The ML Lab is a centralized hub for data science teams to seamlessly build, deploy, and operate machine learning solutions at scale. It is designed to cover the end-to-end machine learning lifecycle from data processing and experimentation to model training and deployment. It combines the libraries, languages, and tools data scientists love, with the infrastructure, services and workflows they need to deliver machine learning solutions into production. More information can be found below. Features \u00b6 Workspace with integrated tooling (Jupyter, Git, Hardware Monitoring, ...) Works with any common machine learning framework (Tensorflow, PyTorch, Sklearn, ...) Upload, manage, version, and share datasets & models Monitor and share experiments for reproducability Deploy models as production-ready services Deployable on a standalone server or on a cluster (via Docker, Kubernetes) and more... Screenshots \u00b6 Try out the walkthrough here . Get Started \u00b6 If you want to install your own ML Lab instance, visit this simple installation guide .","title":"About ML Lab"},{"location":"#welcome-to-machine-learning-lab","text":"The ML Lab is a centralized hub for data science teams to seamlessly build, deploy, and operate machine learning solutions at scale. It is designed to cover the end-to-end machine learning lifecycle from data processing and experimentation to model training and deployment. It combines the libraries, languages, and tools data scientists love, with the infrastructure, services and workflows they need to deliver machine learning solutions into production. More information can be found below.","title":"Welcome to Machine Learning Lab"},{"location":"#features","text":"Workspace with integrated tooling (Jupyter, Git, Hardware Monitoring, ...) Works with any common machine learning framework (Tensorflow, PyTorch, Sklearn, ...) Upload, manage, version, and share datasets & models Monitor and share experiments for reproducability Deploy models as production-ready services Deployable on a standalone server or on a cluster (via Docker, Kubernetes) and more...","title":"Features"},{"location":"#screenshots","text":"Try out the walkthrough here .","title":"Screenshots"},{"location":"#get-started","text":"If you want to install your own ML Lab instance, visit this simple installation guide .","title":"Get Started"},{"location":"empty/","text":"coming soon...","title":"Empty"},{"location":"faq/","text":"FAQ \u00b6 I have problems with my containers. What should I do? The range of issues can be big. A good first thing would be to have a look into the container logs, either via the Service Administration Dashboard (accessible in the web app for admin users) or via command line on the host system. It might also help to just restart you server. For debugging you should remember that ML Lab is an ensemble of various containers, whereas all traffic must pass through the ML Lab backend container. All logs are printed to stdout of the container, so you can see the logs via docker logs lab-backend . This should give you already a good hint about the problem. When the error happens with certain functionalities, for example accessing a tool in the workspace, also have a look at the respective workspace container logs. More logs are shown if you start ML Lab with the environment variable LAB_DEBUG=true (e.g. in Kubernetes-mode, you will see the Kubernetes client requests & responses). A user has a problem with their workspace An ML Lab admin is able to access each user's workspace via the url /workspace/id/{userid} . This allows you to enter a user's workspace and perform debugging there. What if a user reports \"Cannot save file\" error? That error has likely to do something with the filesystem and the lack of available storage. Go to the host where the data is stored and check the available storage via df -h . Afterwards, try to dig deeper with the du -sh /some/path/* (sum the size of the directories) and/or the docker ps -sa (show the sizes of the containers) commands. What to do about unreachable workspaces? Sometimes we observed that a workspace cannot be reached. The logs hint into the direction that not all processes inside the container might have started. So, use the ML Lab's /reset API to remove and recreate the container. If something does not work in Kubernetes, check whether the Deployment and the Service resource belonging to a Workspace still exist, and remove them manually. Then execute the /reset API method again or reload the webapp page to trigger recreation. Please not that the reset method removes the container and re-creates it. It is not the same as restarting. Everything not stored in the workspace's /workspace path will be reset to the workspace image state (including installed libraries etc.). What do I have to know about Data Storage? The ML Lab container does not persist data, but the Minio, Mongo, and workspace containers. In Docker-local, by default, for all of them named Docker volumes are created and mounted. You can pass a host path instead via the HOST_ROOT_DATA_MOUNT_PATH environment variable. If this is set, also the mounts for workspaces have the same host path root; this can be overridden via LAB_DATA_WORKSPACE_ROOT . In managed Kubernetes, a PersistentVolume is created for Minio, Mongo, and the workspace containers. The amount of storage available has to be defined during ML Lab startup via an environment variable. In manual Kubernetes, the node has to be tainted where the Minio, Mongo, and NFS pods always start. They always start on the same node so that they can mount directories from the host file system as in Docker-local. The only difference is that workspaces do not mount the host file system directly but they mount a subdirectory from the started NFS pod. Everything that a user stores in the workspace container under the /workspace directory is persisted on the file system. Everything else is persisted within the container and also consumes disk storage. However, it is deleted when the container is removed; a Docker restart is not enough (ML Lab has a /reset Endpoint for triggering a removal & recreation)! How to create new users (in case self-registration is disabled)? When ML Lab was started via the environment flag ALLOW_SELF_REGISTRATIONS=false , only the admin user can login. Via the REST API (e.g. by using the explorer), the admin can create new users. The endpoint is POST /api/auth/users . Note that the JWT secret which was used for the ML Lab deployment has to be passed here. Does ML Lab capture any metrics? The ML Lab webapp has a management dashboard at /app/#/management containing some metrics. However, the underlying implementation requires a multiple load of the page until all metrics are up to date: load the page, wait a little bit, and reload it again - if needed, repeat a few times. How to clean up ML Lab? The DELETE /api/auth/users/{user} endpoint removes a user from Mongo, its workspace, and its private Minio bucket. Though, it does not remove the persisted volume previously mounted into the user's /workspace ; this has to be done manually on the host. If ML Lab was started with the $LAB_DATA_ROOT or $LAB_DATA_WORKSPACE_ROOT variables, no Docker-volumes are used but an absolute host path that has to be cleaned. The DELETE /api/projects/{projectName} endpoint deletes a project, all belonging experiments and the data from Minio. I cannot login to ML Lab If you get an error message like \"Request has been terminated\" on the login page, this is an indicator that you might have network issues such as being in the wrong network, e.g. when a VPN would be required.","title":"FAQ"},{"location":"faq/#faq","text":"I have problems with my containers. What should I do? The range of issues can be big. A good first thing would be to have a look into the container logs, either via the Service Administration Dashboard (accessible in the web app for admin users) or via command line on the host system. It might also help to just restart you server. For debugging you should remember that ML Lab is an ensemble of various containers, whereas all traffic must pass through the ML Lab backend container. All logs are printed to stdout of the container, so you can see the logs via docker logs lab-backend . This should give you already a good hint about the problem. When the error happens with certain functionalities, for example accessing a tool in the workspace, also have a look at the respective workspace container logs. More logs are shown if you start ML Lab with the environment variable LAB_DEBUG=true (e.g. in Kubernetes-mode, you will see the Kubernetes client requests & responses). A user has a problem with their workspace An ML Lab admin is able to access each user's workspace via the url /workspace/id/{userid} . This allows you to enter a user's workspace and perform debugging there. What if a user reports \"Cannot save file\" error? That error has likely to do something with the filesystem and the lack of available storage. Go to the host where the data is stored and check the available storage via df -h . Afterwards, try to dig deeper with the du -sh /some/path/* (sum the size of the directories) and/or the docker ps -sa (show the sizes of the containers) commands. What to do about unreachable workspaces? Sometimes we observed that a workspace cannot be reached. The logs hint into the direction that not all processes inside the container might have started. So, use the ML Lab's /reset API to remove and recreate the container. If something does not work in Kubernetes, check whether the Deployment and the Service resource belonging to a Workspace still exist, and remove them manually. Then execute the /reset API method again or reload the webapp page to trigger recreation. Please not that the reset method removes the container and re-creates it. It is not the same as restarting. Everything not stored in the workspace's /workspace path will be reset to the workspace image state (including installed libraries etc.). What do I have to know about Data Storage? The ML Lab container does not persist data, but the Minio, Mongo, and workspace containers. In Docker-local, by default, for all of them named Docker volumes are created and mounted. You can pass a host path instead via the HOST_ROOT_DATA_MOUNT_PATH environment variable. If this is set, also the mounts for workspaces have the same host path root; this can be overridden via LAB_DATA_WORKSPACE_ROOT . In managed Kubernetes, a PersistentVolume is created for Minio, Mongo, and the workspace containers. The amount of storage available has to be defined during ML Lab startup via an environment variable. In manual Kubernetes, the node has to be tainted where the Minio, Mongo, and NFS pods always start. They always start on the same node so that they can mount directories from the host file system as in Docker-local. The only difference is that workspaces do not mount the host file system directly but they mount a subdirectory from the started NFS pod. Everything that a user stores in the workspace container under the /workspace directory is persisted on the file system. Everything else is persisted within the container and also consumes disk storage. However, it is deleted when the container is removed; a Docker restart is not enough (ML Lab has a /reset Endpoint for triggering a removal & recreation)! How to create new users (in case self-registration is disabled)? When ML Lab was started via the environment flag ALLOW_SELF_REGISTRATIONS=false , only the admin user can login. Via the REST API (e.g. by using the explorer), the admin can create new users. The endpoint is POST /api/auth/users . Note that the JWT secret which was used for the ML Lab deployment has to be passed here. Does ML Lab capture any metrics? The ML Lab webapp has a management dashboard at /app/#/management containing some metrics. However, the underlying implementation requires a multiple load of the page until all metrics are up to date: load the page, wait a little bit, and reload it again - if needed, repeat a few times. How to clean up ML Lab? The DELETE /api/auth/users/{user} endpoint removes a user from Mongo, its workspace, and its private Minio bucket. Though, it does not remove the persisted volume previously mounted into the user's /workspace ; this has to be done manually on the host. If ML Lab was started with the $LAB_DATA_ROOT or $LAB_DATA_WORKSPACE_ROOT variables, no Docker-volumes are used but an absolute host path that has to be cleaned. The DELETE /api/projects/{projectName} endpoint deletes a project, all belonging experiments and the data from Minio. I cannot login to ML Lab If you get an error message like \"Request has been terminated\" on the login page, this is an indicator that you might have network issues such as being in the wrong network, e.g. when a VPN would be required.","title":"FAQ"},{"location":"getting-started/","text":"Getting Started in 30 seconds \u00b6 You just found out about ML Lab and want to try it out? Congrats , this is officially the fastest way to get you started. To get started, you have two options: Getting Started in 30 seconds Install ML Lab Next steps At your own risk! Even so ML Lab is, of course, suuuper stable \ud83d\ude0e it is intended for ML Experimentation and Model Testing, not necessarily for production-critical deployment. Install ML Lab \u00b6 To install ML Lab, all you need is a single machine, preferably Mac or Linux, and a few seconds of your precious time ... ok maybe a few minutes, but it's worth the time . Wait... there is one requirement ML Lab requires Docker to be installed on your host machine. Docker is running on your machine? Perfect! Now everything is ready for the big moment Just run this command: docker run --rm --env LAB_ACTION=install -v /var/run/docker.sock:/var/run/docker.sock --env LAB_PORT=8091 lab-service:latest Yeay, that was easy The ML Lab install process will automatically download and install everything it needs. This may take a few minutes depending on your internet speed. Grab a coffee and take a few minutes to dream about all the exciting things you can do with this tool . After some minutes, is the Web UI finally loading on http://<HOSTIP>:8091 ? Yes: Congratulations , everything is set up and you are ready for next steps ! No: Sorry , there might be an error with the installation. Please check the logs and refer to this installation guide for more details. Next steps \u00b6 ML Lab is ready to go Now what? Walkthrough : A visual tutorial that helps you explore all of the best functionalities. Full Installation Guide : A guide that helps you to deploy ML Lab to production, so that everyone on your team can get first-class access.","title":"Getting Started"},{"location":"getting-started/#getting-started-in-30-seconds","text":"You just found out about ML Lab and want to try it out? Congrats , this is officially the fastest way to get you started. To get started, you have two options: Getting Started in 30 seconds Install ML Lab Next steps At your own risk! Even so ML Lab is, of course, suuuper stable \ud83d\ude0e it is intended for ML Experimentation and Model Testing, not necessarily for production-critical deployment.","title":"Getting Started in 30 seconds"},{"location":"getting-started/#install-ml-lab","text":"To install ML Lab, all you need is a single machine, preferably Mac or Linux, and a few seconds of your precious time ... ok maybe a few minutes, but it's worth the time . Wait... there is one requirement ML Lab requires Docker to be installed on your host machine. Docker is running on your machine? Perfect! Now everything is ready for the big moment Just run this command: docker run --rm --env LAB_ACTION=install -v /var/run/docker.sock:/var/run/docker.sock --env LAB_PORT=8091 lab-service:latest Yeay, that was easy The ML Lab install process will automatically download and install everything it needs. This may take a few minutes depending on your internet speed. Grab a coffee and take a few minutes to dream about all the exciting things you can do with this tool . After some minutes, is the Web UI finally loading on http://<HOSTIP>:8091 ? Yes: Congratulations , everything is set up and you are ready for next steps ! No: Sorry , there might be an error with the installation. Please check the logs and refer to this installation guide for more details.","title":"Install ML Lab"},{"location":"getting-started/#next-steps","text":"ML Lab is ready to go Now what? Walkthrough : A visual tutorial that helps you explore all of the best functionalities. Full Installation Guide : A guide that helps you to deploy ML Lab to production, so that everyone on your team can get first-class access.","title":"Next steps"},{"location":"administration/backup-data/","text":"Backup Data \u00b6 To prevent full data loss, it is important to have periodic backups of the volumes or directories mounted into the following services: Minio Storage ( lab-minio : /data directory) Mongo Storage ( lab-mongo : /data/db directory) Workspace Backups The /workspace directory within all Lab-managed Workspaces is automatically backed up to the Minio storage every day at approx. 4am. However, all files in the /environment folder, as well as all files with a size of more than 50MB, will be ignored in the automatic backup. Lab will keep only the last three Workspace backups for every user. In the case of data loss, the last backup - if available - will be automatically restored in the startup process of the Workspace. We recommend using Volumerize to backup the persisted data of the Minio and Mongo Storage to one of the many supported backends (e.g. filesystem, ssh, rsync, s3). With Volumerize, it is also possible to schedule periodic backups. Please refer to the Volumerize documentation for all features and usage information. In the following two sections, we will show examples on how to use Volumerize to backup persisted data from a Docker-local and Kubernetes Lab instance. Example: Docker Local \u00b6 Backup lab-minio and lab-mongo volumes every 2 days at 2am into the local folder that the following command is executed from ( $(pwd) ): docker run -d --restart always --name lab-backup \\ -v lab-minio:/source/lab-minio:ro \\ -v lab-mongo:/source/lab-mongo:ro \\ -v lab-backup-cache:/volumerize-cache \\ -v $( pwd ) :/backup \\ -e \"VOLUMERIZE_SOURCE=/source\" \\ -e \"VOLUMERIZE_TARGET=file:///backup\" \\ -e \"VOLUMERIZE_DUPLICITY_OPTIONS=--progress\" \\ -e \"TZ=Europe/Berlin\" \\ -e \"VOLUMERIZE_JOBBER_TIME=0 0 3 */2 * *\" \\ blacklabelops/volumerize:1.5.0 Example: Kubernetes \u00b6 In a Kubernetes Lab instance, the persisted data is on the manager node's filesystem in the folder specified with LAB_DATA_ROOT during the installation process. In the following example, the LAB_DATA_ROOT is /lab/data/ which means that the Minio data is persisted at /lab/data/lab-minio and the Mongo data at /lab/data/lab-mongo on the file system of the manager node. The backup is scheduled for every 2 days at 3am as well. Instead of backing up the data on the local filesystem (as shown in the Docker local example) we will use rsync for this example to move the data to a remote machine specified via rsync://10.12.345.678//lab/backup . In order to be able to connect, valid key information needs to be provided ( id_rsa and known_hosts ) as demonstrated below: docker run -d -restart always --name lab-backup \\ --name volumerize-backup \\ -v /lab/data/lab-minio:/source/lab-minio:ro \\ -v /lab/data/lab-mongo:/source/lab-mongo:ro \\ -v lab-backup-cache:/volumerize-cache \\ -v /lab/backup/backup_key:/root/.ssh/id_rsa \\ -v /root/.ssh/known_hosts:/root/.ssh/known_hosts \\ -e \"VOLUMERIZE_SOURCE=/source\" \\ -e \"VOLUMERIZE_TARGET=rsync://10.12.345.678//lab/backup\" \\ -e \"VOLUMERIZE_DUPLICITY_OPTIONS=--progress\" \\ -e \"TZ=Europe/Berlin\" \\ -e \"VOLUMERIZE_JOBBER_TIME=0 0 3 */2 * *\" \\ blacklabelops/volumerize:1.5.0","title":"Backup Data"},{"location":"administration/backup-data/#backup-data","text":"To prevent full data loss, it is important to have periodic backups of the volumes or directories mounted into the following services: Minio Storage ( lab-minio : /data directory) Mongo Storage ( lab-mongo : /data/db directory) Workspace Backups The /workspace directory within all Lab-managed Workspaces is automatically backed up to the Minio storage every day at approx. 4am. However, all files in the /environment folder, as well as all files with a size of more than 50MB, will be ignored in the automatic backup. Lab will keep only the last three Workspace backups for every user. In the case of data loss, the last backup - if available - will be automatically restored in the startup process of the Workspace. We recommend using Volumerize to backup the persisted data of the Minio and Mongo Storage to one of the many supported backends (e.g. filesystem, ssh, rsync, s3). With Volumerize, it is also possible to schedule periodic backups. Please refer to the Volumerize documentation for all features and usage information. In the following two sections, we will show examples on how to use Volumerize to backup persisted data from a Docker-local and Kubernetes Lab instance.","title":"Backup Data"},{"location":"administration/backup-data/#example-docker-local","text":"Backup lab-minio and lab-mongo volumes every 2 days at 2am into the local folder that the following command is executed from ( $(pwd) ): docker run -d --restart always --name lab-backup \\ -v lab-minio:/source/lab-minio:ro \\ -v lab-mongo:/source/lab-mongo:ro \\ -v lab-backup-cache:/volumerize-cache \\ -v $( pwd ) :/backup \\ -e \"VOLUMERIZE_SOURCE=/source\" \\ -e \"VOLUMERIZE_TARGET=file:///backup\" \\ -e \"VOLUMERIZE_DUPLICITY_OPTIONS=--progress\" \\ -e \"TZ=Europe/Berlin\" \\ -e \"VOLUMERIZE_JOBBER_TIME=0 0 3 */2 * *\" \\ blacklabelops/volumerize:1.5.0","title":"Example: Docker Local"},{"location":"administration/backup-data/#example-kubernetes","text":"In a Kubernetes Lab instance, the persisted data is on the manager node's filesystem in the folder specified with LAB_DATA_ROOT during the installation process. In the following example, the LAB_DATA_ROOT is /lab/data/ which means that the Minio data is persisted at /lab/data/lab-minio and the Mongo data at /lab/data/lab-mongo on the file system of the manager node. The backup is scheduled for every 2 days at 3am as well. Instead of backing up the data on the local filesystem (as shown in the Docker local example) we will use rsync for this example to move the data to a remote machine specified via rsync://10.12.345.678//lab/backup . In order to be able to connect, valid key information needs to be provided ( id_rsa and known_hosts ) as demonstrated below: docker run -d -restart always --name lab-backup \\ --name volumerize-backup \\ -v /lab/data/lab-minio:/source/lab-minio:ro \\ -v /lab/data/lab-mongo:/source/lab-mongo:ro \\ -v lab-backup-cache:/volumerize-cache \\ -v /lab/backup/backup_key:/root/.ssh/id_rsa \\ -v /root/.ssh/known_hosts:/root/.ssh/known_hosts \\ -e \"VOLUMERIZE_SOURCE=/source\" \\ -e \"VOLUMERIZE_TARGET=rsync://10.12.345.678//lab/backup\" \\ -e \"VOLUMERIZE_DUPLICITY_OPTIONS=--progress\" \\ -e \"TZ=Europe/Berlin\" \\ -e \"VOLUMERIZE_JOBBER_TIME=0 0 3 */2 * *\" \\ blacklabelops/volumerize:1.5.0","title":"Example: Kubernetes"},{"location":"administration/external-oidc-authentication/","text":"External OIDC Authentication \u00b6 By default, ML Lab offers authentication via username and password. Users are either allowed to self-register or an admin needs to create their account and set an initial password. This approach is simple but not always the safest as users might choose weak passwords or do not change them regularly. That is why ML Lab allows the configuration of an external OIDC authentication provider (Azure AD, GitHub, etc.) to delegate the authentication to a third party. OIDC is a simple identity layer on top of the OAuth 2.0 protocol and is supported by many authentication providers. The ML Lab Authentication Flow \u00b6 The sequence diagram above shows how ML Lab interacts with the external OIDC authentication provider. After the user visits the login page, the browser is redirected to the external OIDC authentication endpoint which was configured via the environment variable LAB_EXTERNAL_OIDC_AUTH_URL . Once the user authenticated successfully, the browser is redirected back to ML Lab backend. It uses the configured external OIDC token URL (environment variable LAB_EXTERNAL_OIDC_TOKEN_URL ), the client ID (environment variable LAB_EXTERNAL_OIDC_CLIENT_ID ) and client secret (environment variable LAB_EXTERNAL_OIDC_CLIENT_SECRET ) to request the user's OIDC identity token. This token contains the user's e-mail address which functions as the username. An ML Lab JWT token is set as a cookie in the user's browser so ML Lab can be used without further authentication. Setup external OIDC authentication with Azure AD \u00b6 Open the Azure portal, navigate to the Azure Active Directory\" page and select \"App registration\" from the left side menu. Click on the \"Endpoints\" tab. The value shown for \"OAuth 2.0 authorization endpoint (v2)\" must be used for the LAB_EXTERNAL_OIDC_AUTH_URL environment variable. The value shown for \"OAuth 2.0 token endpoint (v2)\" must be used for the LAB_EXTERNAL_OIDC_TOKEN_URL environment variable. Click on the tab \"New registration\" and create a new app registration for ML Lab. On the \"Overview\" page, copy the value of the \"Application (client) ID\" field. It will be used for the LAB_EXTERNAL_OIDC_CLIENT_ID environment variable. On the left side menu choose \"Certificates & secrets\" and click on \"New client secret\". Choose a name and expiration time. After that, copy the value of new newly created client secret. It will be use for the LAB_EXTERNAL_OIDC_CLIENT_SECRET environment variable. The \"ID\" field is not required. On the left side menu choose \"Authentication\" and click on \"Add a platform\". There choose the \"Web\" platform and add the callback URL of your ML Lab instance (https://<insert-your-host>/api/auth/oidc/callback). Save your changes afterwards. Now you can install the ML Lab like described here and set the required environment variables. Setup external OIDC authentication with Dex \u00b6 Dex is and identity service that uses OpenID Connect (OIDC) and provides a range of connectors to other authentication methods like SAML 2.0, LDAP, GitHub Oauth 2.0 and many more. Detailed information on how to setup Dex can be found here: https://github.com/dexidp/dex It can be run as a docker container alongside ML Lab. In the dex configuration file the client id and secret can be specified. After starting Dex, the authentication and token URL can be found at /dex/.well-known/openid-configuration .","title":"External OIDC Authentication"},{"location":"administration/external-oidc-authentication/#external-oidc-authentication","text":"By default, ML Lab offers authentication via username and password. Users are either allowed to self-register or an admin needs to create their account and set an initial password. This approach is simple but not always the safest as users might choose weak passwords or do not change them regularly. That is why ML Lab allows the configuration of an external OIDC authentication provider (Azure AD, GitHub, etc.) to delegate the authentication to a third party. OIDC is a simple identity layer on top of the OAuth 2.0 protocol and is supported by many authentication providers.","title":"External OIDC Authentication"},{"location":"administration/external-oidc-authentication/#the-ml-lab-authentication-flow","text":"The sequence diagram above shows how ML Lab interacts with the external OIDC authentication provider. After the user visits the login page, the browser is redirected to the external OIDC authentication endpoint which was configured via the environment variable LAB_EXTERNAL_OIDC_AUTH_URL . Once the user authenticated successfully, the browser is redirected back to ML Lab backend. It uses the configured external OIDC token URL (environment variable LAB_EXTERNAL_OIDC_TOKEN_URL ), the client ID (environment variable LAB_EXTERNAL_OIDC_CLIENT_ID ) and client secret (environment variable LAB_EXTERNAL_OIDC_CLIENT_SECRET ) to request the user's OIDC identity token. This token contains the user's e-mail address which functions as the username. An ML Lab JWT token is set as a cookie in the user's browser so ML Lab can be used without further authentication.","title":"The ML Lab Authentication Flow"},{"location":"administration/external-oidc-authentication/#setup-external-oidc-authentication-with-azure-ad","text":"Open the Azure portal, navigate to the Azure Active Directory\" page and select \"App registration\" from the left side menu. Click on the \"Endpoints\" tab. The value shown for \"OAuth 2.0 authorization endpoint (v2)\" must be used for the LAB_EXTERNAL_OIDC_AUTH_URL environment variable. The value shown for \"OAuth 2.0 token endpoint (v2)\" must be used for the LAB_EXTERNAL_OIDC_TOKEN_URL environment variable. Click on the tab \"New registration\" and create a new app registration for ML Lab. On the \"Overview\" page, copy the value of the \"Application (client) ID\" field. It will be used for the LAB_EXTERNAL_OIDC_CLIENT_ID environment variable. On the left side menu choose \"Certificates & secrets\" and click on \"New client secret\". Choose a name and expiration time. After that, copy the value of new newly created client secret. It will be use for the LAB_EXTERNAL_OIDC_CLIENT_SECRET environment variable. The \"ID\" field is not required. On the left side menu choose \"Authentication\" and click on \"Add a platform\". There choose the \"Web\" platform and add the callback URL of your ML Lab instance (https://<insert-your-host>/api/auth/oidc/callback). Save your changes afterwards. Now you can install the ML Lab like described here and set the required environment variables.","title":"Setup external OIDC authentication with Azure AD"},{"location":"administration/external-oidc-authentication/#setup-external-oidc-authentication-with-dex","text":"Dex is and identity service that uses OpenID Connect (OIDC) and provides a range of connectors to other authentication methods like SAML 2.0, LDAP, GitHub Oauth 2.0 and many more. Detailed information on how to setup Dex can be found here: https://github.com/dexidp/dex It can be run as a docker container alongside ML Lab. In the dex configuration file the client id and secret can be specified. After starting Dex, the authentication and token URL can be found at /dex/.well-known/openid-configuration .","title":"Setup external OIDC authentication with Dex"},{"location":"administration/manage-services/","text":"Manage Services \u00b6 The service-admin section provides an administration dashboard (via Portainer in local or Swarm mode or Kubernetes Dashboard in Kubernetes mode) that can be used by an admin user to manage the Docker resources using a GUI without the need to use the command-line interface. Info The Service Admin Dashboard is only accesible if you are logged into Lab as admin user. For more detailed information on how to manage services, go to the official Portainer page and visit the Docker documentation or the Kubernetes documentation .","title":"Manage Services"},{"location":"administration/manage-services/#manage-services","text":"The service-admin section provides an administration dashboard (via Portainer in local or Swarm mode or Kubernetes Dashboard in Kubernetes mode) that can be used by an admin user to manage the Docker resources using a GUI without the need to use the command-line interface. Info The Service Admin Dashboard is only accesible if you are logged into Lab as admin user. For more detailed information on how to manage services, go to the official Portainer page and visit the Docker documentation or the Kubernetes documentation .","title":"Manage Services"},{"location":"administration/uninstall-lab/","text":"Uninstall ML Lab \u00b6 All docker resources of an ML Lab installation are labeled by the used LAB_NAMESPACE (default= lab ). These docker resources related to a ML Lab installation (regardless if it is installed in docker, or kubernetes mode) can be removed via a convenient docker run command: docker run --rm --env LAB_ACTION = uninstall -v /var/run/docker.sock:/var/run/docker.sock lab-service:latest All resources started outside of the ML Lab instance, e.g. Backup Jobs , need to be removed manually. If you have used another LAB_NAMESPACE than the default one ( lab ), you need to specify this namespace as well ( --env LAB_NAMESPACE=\"YOUR_NAMESPACE\" ). Warning This un-installation process will also remove all volumes and thereby delete all data of your installation.","title":"Uninstall ML Lab"},{"location":"administration/uninstall-lab/#uninstall-ml-lab","text":"All docker resources of an ML Lab installation are labeled by the used LAB_NAMESPACE (default= lab ). These docker resources related to a ML Lab installation (regardless if it is installed in docker, or kubernetes mode) can be removed via a convenient docker run command: docker run --rm --env LAB_ACTION = uninstall -v /var/run/docker.sock:/var/run/docker.sock lab-service:latest All resources started outside of the ML Lab instance, e.g. Backup Jobs , need to be removed manually. If you have used another LAB_NAMESPACE than the default one ( lab ), you need to specify this namespace as well ( --env LAB_NAMESPACE=\"YOUR_NAMESPACE\" ). Warning This un-installation process will also remove all volumes and thereby delete all data of your installation.","title":"Uninstall ML Lab"},{"location":"administration/update-lab/","text":"Update Lab \u00b6 Updating the ML Lab landscape is a tricky process, as it consists of a variety of Docker containers. To make it simple, we provide a convenient docker run action ( LAB_ACTION=update ) to update a Lab instance to a specific version. Use Installation Configuration! For the update action, you should use the same configuration as used for the installed Lab instance. Especially, the JWT_SECRET , SERVICES_RUNTIME , and SERVICE_SSL_ENABLED are required to be the same values as used by the installed instance. Refer to the installation section for all possible configuration parameters. To update, figure out the configuration of your installed Lab instance and run: docker run --rm -v /var/run/docker.sock:/var/run/docker.sock --env LAB_ACTION = update lab-service:<NEW_VERSION>","title":"Update ML Lab"},{"location":"administration/update-lab/#update-lab","text":"Updating the ML Lab landscape is a tricky process, as it consists of a variety of Docker containers. To make it simple, we provide a convenient docker run action ( LAB_ACTION=update ) to update a Lab instance to a specific version. Use Installation Configuration! For the update action, you should use the same configuration as used for the installed Lab instance. Especially, the JWT_SECRET , SERVICES_RUNTIME , and SERVICE_SSL_ENABLED are required to be the same values as used by the installed instance. Refer to the installation section for all possible configuration parameters. To update, figure out the configuration of your installed Lab instance and run: docker run --rm -v /var/run/docker.sock:/var/run/docker.sock --env LAB_ACTION = update lab-service:<NEW_VERSION>","title":"Update Lab"},{"location":"architecture/architecture/","text":"Architecture \u00b6 The following figure displays the different resource types managed by the ML Lab container. The resources Minio , Mongo , and NFS exist once. All traffic goes through the ML Lab container. It was designed to scale horizontally, although this functionality has not been thoroughly tested yet. So far, no performance issues were observed as the heavy load happens inside the Service or Workspace containers, such as running Python code. Workspace containers are started for each user. Indifinetely Services and Jobs can exist within each project, only limited by the underlying infrastructure. In Kubernetes, an admin can theoretically scale a single Service manually. By ML Lab created resources follow a naming pattern and are assigned specific labels (Docker & Kubernetes) that assign them to specific projects. By doing so, the capabilities of the underlying infrastructure technology is leveraged. The next figure visualizes the most important components within the ML Lab container. Inside of the ML Lab container, the nginx proxy is the first request handler. It determines whether the request goes to the Java backend, which is also hosted inside of the ML Lab container, or whether it goes to another container. The following diagram shows the routing of requests with respect to the ML Lab container in more detail. The following figure gives an high-level overview of the concepts that exist withtin the ML Lab world.","title":"ML Lab Architecture"},{"location":"architecture/architecture/#architecture","text":"The following figure displays the different resource types managed by the ML Lab container. The resources Minio , Mongo , and NFS exist once. All traffic goes through the ML Lab container. It was designed to scale horizontally, although this functionality has not been thoroughly tested yet. So far, no performance issues were observed as the heavy load happens inside the Service or Workspace containers, such as running Python code. Workspace containers are started for each user. Indifinetely Services and Jobs can exist within each project, only limited by the underlying infrastructure. In Kubernetes, an admin can theoretically scale a single Service manually. By ML Lab created resources follow a naming pattern and are assigned specific labels (Docker & Kubernetes) that assign them to specific projects. By doing so, the capabilities of the underlying infrastructure technology is leveraged. The next figure visualizes the most important components within the ML Lab container. Inside of the ML Lab container, the nginx proxy is the first request handler. It determines whether the request goes to the Java backend, which is also hosted inside of the ML Lab container, or whether it goes to another container. The following diagram shows the routing of requests with respect to the ML Lab container in more detail. The following figure gives an high-level overview of the concepts that exist withtin the ML Lab world.","title":"Architecture"},{"location":"architecture/machine-learning-lifecycle/","text":"Machine Learning Lifecycle \u00b6 The Machine Learning Lifecycle can be unnecessary cumbersome. If you are familiar with data science projects, you were most likely confronted with various of these questions: How to provide a multi-user environment for doing machine learning experiments? How to efficiently provide computing resources to the data science team? How to store datasets and trained models? How to organize and monitor experiments? How to deploy models and expose models via REST API in a scalable and secure way? How to provide GPU access to data science teams? ML Lab enables teams to be more productive in delivering machine learning technologies for their products and datasets. This platform helps with a variety of common tasks within the machine learning lifecycle as described below. The same general workflow exists across almost all machine learning use cases:","title":"ML Lifecycle"},{"location":"architecture/machine-learning-lifecycle/#machine-learning-lifecycle","text":"The Machine Learning Lifecycle can be unnecessary cumbersome. If you are familiar with data science projects, you were most likely confronted with various of these questions: How to provide a multi-user environment for doing machine learning experiments? How to efficiently provide computing resources to the data science team? How to store datasets and trained models? How to organize and monitor experiments? How to deploy models and expose models via REST API in a scalable and secure way? How to provide GPU access to data science teams? ML Lab enables teams to be more productive in delivering machine learning technologies for their products and datasets. This platform helps with a variety of common tasks within the machine learning lifecycle as described below. The same general workflow exists across almost all machine learning use cases:","title":"Machine Learning Lifecycle"},{"location":"installation/install-lab/","text":"ML Lab Installation \u00b6 Preparation \u00b6 Install Docker ML Lab requires Docker to be installed on your host machine. Choose Service Runtime \u00b6 ML Lab installs and orchestrates various services in the form of Docker container. At the moment, we offer the following installation modes: Docker Local Mode: Deploys all services on the same machine as ML Lab. Easy to setup and manage, but does not scale. Kubernetes Mode: Distributes all services across a cluster of nodes via Kubernetes. For more information about Kubernetes, please refer to the official guide . Tip If you are not sure which mode to use, we recommend the local mode . Installation \u00b6 If you have special requirements (e.g. data persistance, ssl, hardware restrictions), please consult the configuration section before installing ML Lab. Configuration \u00b6 Parameters \u00b6 The container can be configured with following environment variables ( --env ): Variable Description Default LAB_ACTION Available actions: install, uninstall, serve, update, update-full install LAB_PORT Main port that the ML Lab instance is accesible from. 8091 LAB_BASE_URL If you deploy ML Lab behind a proxy, you can define a base url. It must not end with a slash. For example, if the web app should be accessible behind /test/app instead of /app , the LAB_BASE_URL would be defined as /test . SERVICES_RUNTIME Determines the technology used for container orchastration. Currently supported: local, kubernetes local JWT_SECRET The secret used for the authentication layer. (required); at least 32 characters long SERVICE_SSL_ENABLED Set to true to enable ssl encryption (HTTPS support). If no certificate is provided, a self-signed certificate is generated; this certificate expires within a year, so the ML Lab container has to be re-created for fresh certificates. false LAB_SSL_ROOT The path on the host system to the folder containing a custom ssl certificate. The folder must contain a cert.crt and cert.key file. This folder is mounted into the ML Lab container, so to renew the certificate it must be replaced on the host system and the ML Lab container has to be restarted. If the LAB_SSL_ROOT variable is not set, ML Lab will look for a volume named lab_ssl. If no such named volume exists, a self signed certificate will be generated. The LAB_SSL_ROOT variable is only valid for docker local mode as secretes are used on Kubernetes. (optional) LAB_NAMESPACE The namespace used for the ML Lab installation. At the moment, we suggest to not change this value. lab SERVICES_MEMORY_LIMIT The memory limit (in GB) that every ML Lab managed service, including workspaces, is restricted to. 100 SERVICES_CPU_LIMIT The CPU limit (number of CPUs) that every ML Lab managed service, including workspaces, is restricted to. 8 SERVICES_STORAGE_LIMIT The storage limit (in GB) that every ML Lab managed service, including workspaces, is restricted to. For Docker-local and Kubernetes custom cluster setup, there is only a soft enforcement in the workspace. In case of the Kubernetes managed cluster setup, a volume with this size is created on the respective infrastructure and mounted into the service pod (check this as this can lead to substantial costs!). 100 MAX_CONTAINER_SIZE The maximum size a ML Lab managed container is allowed to grow in Gigabytes. If you add data to a container's writeable layer - basically a path where no volume is mounted - the container size grows. If not set, a container can theoretically consume all of the host's storage. For Docker-local, ML Lab contains a REST-method (/containers/shutdown-disk-exceeding) that will remove all non-core containers that exceed this limit (in Docker-local mode, currently only workspaces are removed). In Kubernetes mode, the native functionality of ephemeral-storage limit is used (for all non-core pods). Set to '-1' to disable it. 100 LAB_DATA_ROOT Basic mount path where all data is stored. (optional) LAB_DATA_WORKSPACE_ROOT Basic mount path where all workspace data is stored. Overwrites the LAB_DATA_ROOT variable for workspace mount. (optional) LAB_DEBUG If true, ML Lab will expose all ports and print out debug logs. false LAB_SSH_ENABLED Enable ssh jumphost if ML Lab should publish port 22 on startup and start an SSH server. The jumphost functionality can be used so that users can ssh into their own workspace. SSHing into the workspace container itself is not possible. true ALLOW_SELF_REGISTRATIONS If true, ML Lab will allow user self registrations via register dialog or automatically create users if external OIDC authentication is enabled. true WORKSPACE_BACKUP If true, workspaces will be automatically backuped every day to the ML Lab Storage and restored if necessary. false WORKSPACE_IMAGE Docker image used for user workspaces. Should be build on top of the ml-workspace base image. (optional) LAB_MANAGED_KUBERNETES Specifies whether it is running on a managed Kubernetes cluster instance. For more information, please have a look at the Section about Kubernetes managed cluster setup. false LAB_STORAGE_CLASS Only relevant for Kubernetes managed cluster setup. The storage class name that is used by the persistent volume claims for issuing the volumes mounted into the pods of Minio, Mongo, and the Workspaces. lab-storageclass LAB_PVC_MINIO_STORAGE_LIMIT Only relevant for Kubernetes managed cluster setup. It defines the size of the volume created and mounted into the Minio pod in GB. Minio is the storage for uploaded files such as datasets and models. 100 LAB_PVC_MONGO_STORAGE_LIMIT Only relevant for Kubernetes managed cluster setup. It defines the size of the volume created and mounted into the Mongo pod in GB. Mongo contains the data for experiments and users. 5 LAB_IMAGE_REGISTRY The registry prefix from where the images lab-service and lab-model-service are loaded. If you, for example, deploy it in Azure you might not have access to the internal Artifactory registry. Make sure to push the images to the defined registry. If you don't use the one-click model-deployment feature, you don't have to push the lab-model-service image. Default DockerHub registry LAB_EXTERNAL_OIDC_AUTH_URL The authorization endpoint used for external OIDC authentication. The client will be redirected to this page to authenticate with the external authentication provider. For detailed information see: External OIDC Authentication (optional) LAB_EXTERNAL_OIDC_TOKEN_URL The token endpoint used for external OIDC authentication. It will be used by the backend to obtain the OIDC identity token in exchange for an authorization code. For detailed information see: External OIDC Authentication (optional) LAB_EXTERNAL_OIDC_CLIENT_ID The OAuth 2.0 client identifier used for external OIDC authentication. For detailed information see: External OIDC Authentication (optional) LAB_EXTERNAL_OIDC_CLIENT_SECRET The OAuth 2.0 client secret used for external OIDC authentication. For detailed information see: External OIDC Authentication (optional) Proxy \u00b6 If a proxy is required, you can pass the proxy configuration via the http_proxy and no_proxy environment variables. For example: --env http_proxy=http://myproxy:1234 Install with Docker Local Mode \u00b6 Note: The install commands here are currently based on the scenario where you built the Docker images locally yourself. For seeing how to use ready images, please check the Readme of the GitHub repository here To start ML Lab in a single-host (local) deployment execute: docker run --rm --env LAB_PORT = 8091 -v /var/run/docker.sock:/var/run/docker.sock --env LAB_ACTION = install lab-service:latest After the installation is finished (after several minutes depending on internet speed), visit http://<HOSTIP>:8091 and login with admin:admin . Enable SSL For SSL setup, create the certificate (files must be called cert.crt and cert.key) and specify their path on the host machine via the LAB_SSL_ROOT environment variable. Additionally you need to set SERVICE_SSL_ENABLED to true: docker run --rm --env LAB_PORT = 8091 \\ --env SERVICE_SSL_ENABLED = true \\ --env LAB_SSL_ROOT = /workspace/ssl \\ -v /var/run/docker.sock:/var/run/docker.sock \\ lab-service:latest Alternatively, instead of specifying LAB_SSL_ROOT , the certificate can be provided in a docker volume named lab_ssl . If you don't provide a custom certificate, a self-signed certificate is generated and used. Be aware that applications such as your browser might not trust the certificate. Install with Kubernetes Mode (Own Cluster) \u00b6 These steps are for a custom cluster that does not use a hyperscaler / automated infrastructure in the background. If you have a managed cluster (such as AWS EKS), then please follow the steps in the next Section. Install Kubernetes For Kubernetes Mode, please make sure that your server/cluster is installed with Docker and Kubernetes (Version >=1.11). You can find more information about Kubernetes installation here . Preparation For the Kubernetes deployment, a few minor steps to prepare the host have to be made such as creating a directory where the data is stored etc. since Kubernetes does not have the concept of Docker's named volumes. The Kubernetes version of the ML Lab needs the kube config of the cluster as well as the access to the Docker socket. # label the master node with role=master, as we use it in ML Lab kubectl label nodes <name-of-master-node> role = master # install nfs-common for mounting nfs-service in workspace apt-get install nfs-common # create the data root directory that will be used by Kubernetes # Default is `/workspace/lab/<namespace>/data` # Hence, when creating a Lab for default namespace 'lab': mkdir -p /workspace/lab/lab/data Installation # On Mac: in ~/.kube/config for 'server' field replace 'localhost' with 'docker.for.mac.localhost' docker run --rm --env LAB_PORT = 30001 \\ -v /root/.kube/config:/root/.kube/config \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --env SERVICES_RUNTIME = k8s \\ --env LAB_DATA_ROOT = /workspace/lab/stulabdio/data \\ lab-service:latest After installation is finished (after several minutes depending on intranet speed), visit http://<HOSTIP>:30001 and login with admin:admin . Tip When a container, e.g. the Workspace container, is launched on a node the first time, it's Docker image has to be pulled to that node. This can take some time in which the user sees a loading screen. To prevent this, you can pull the images manually for each node beforehand. Enable SSL For SSL setup, create the certificates and mount them into the container's directory at /resources/ssl ( -v /workspace/ssl:/resources/ssl:ro ) and start with --env SERVICE_SSL_ENABLED=true : docker run --rm --env LAB_PORT = 30001 \\ -v /root/.kube/config:/root/.kube/config \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --env SERVICES_RUNTIME = k8s \\ --env LAB_DATA_ROOT = /workspace/lab/lab/data \\ --env SERVICE_SSL_ENABLED = true \\ -v /workspace/ssl:/resources/ssl:ro \\ lab-service:latest The files have to be named cert.crt and cert.key . Install with Kubernetes (Managed Cluster such as AWS EKS) \u00b6 TODO Difference to Custom Cluster Setup \u00b6 In the Managed Cluster scenario, the biggest difference is that you don't have to take care of the volumes as here we leverage persistent volume (claims) to automatically issue volumes in the background. Hence, no need for the NFS service here. In this setup mode, ML Lab accesses the cluster via its ServiceAccount permissions and not via a mounted kube-config. Costs As volumes are created automatically on the infrastructure based on the Kubernetes persistent volume claims, make sure to have an eye on your costs and set the size for the volume via the respective environment variables accordingly. Even when deleting the Kuberentes PersistentVolume and PersistentVolumeClaim resources, the actual volumes might still exist on the cloud provider and have to be deleted manually. After Installation \u00b6 Please makes sure to change the admin password ( User-menu -> Change Password ) after the installation was successful. We also recommend to activate a data backup job as described here . Check out the administration section in this documentation for more information on how to update / uninstall Lab, or manage services .","title":"Installation"},{"location":"installation/install-lab/#ml-lab-installation","text":"","title":"ML Lab Installation"},{"location":"installation/install-lab/#preparation","text":"Install Docker ML Lab requires Docker to be installed on your host machine.","title":"Preparation"},{"location":"installation/install-lab/#choose-service-runtime","text":"ML Lab installs and orchestrates various services in the form of Docker container. At the moment, we offer the following installation modes: Docker Local Mode: Deploys all services on the same machine as ML Lab. Easy to setup and manage, but does not scale. Kubernetes Mode: Distributes all services across a cluster of nodes via Kubernetes. For more information about Kubernetes, please refer to the official guide . Tip If you are not sure which mode to use, we recommend the local mode .","title":"Choose Service Runtime"},{"location":"installation/install-lab/#installation","text":"If you have special requirements (e.g. data persistance, ssl, hardware restrictions), please consult the configuration section before installing ML Lab.","title":"Installation"},{"location":"installation/install-lab/#configuration","text":"","title":"Configuration"},{"location":"installation/install-lab/#parameters","text":"The container can be configured with following environment variables ( --env ): Variable Description Default LAB_ACTION Available actions: install, uninstall, serve, update, update-full install LAB_PORT Main port that the ML Lab instance is accesible from. 8091 LAB_BASE_URL If you deploy ML Lab behind a proxy, you can define a base url. It must not end with a slash. For example, if the web app should be accessible behind /test/app instead of /app , the LAB_BASE_URL would be defined as /test . SERVICES_RUNTIME Determines the technology used for container orchastration. Currently supported: local, kubernetes local JWT_SECRET The secret used for the authentication layer. (required); at least 32 characters long SERVICE_SSL_ENABLED Set to true to enable ssl encryption (HTTPS support). If no certificate is provided, a self-signed certificate is generated; this certificate expires within a year, so the ML Lab container has to be re-created for fresh certificates. false LAB_SSL_ROOT The path on the host system to the folder containing a custom ssl certificate. The folder must contain a cert.crt and cert.key file. This folder is mounted into the ML Lab container, so to renew the certificate it must be replaced on the host system and the ML Lab container has to be restarted. If the LAB_SSL_ROOT variable is not set, ML Lab will look for a volume named lab_ssl. If no such named volume exists, a self signed certificate will be generated. The LAB_SSL_ROOT variable is only valid for docker local mode as secretes are used on Kubernetes. (optional) LAB_NAMESPACE The namespace used for the ML Lab installation. At the moment, we suggest to not change this value. lab SERVICES_MEMORY_LIMIT The memory limit (in GB) that every ML Lab managed service, including workspaces, is restricted to. 100 SERVICES_CPU_LIMIT The CPU limit (number of CPUs) that every ML Lab managed service, including workspaces, is restricted to. 8 SERVICES_STORAGE_LIMIT The storage limit (in GB) that every ML Lab managed service, including workspaces, is restricted to. For Docker-local and Kubernetes custom cluster setup, there is only a soft enforcement in the workspace. In case of the Kubernetes managed cluster setup, a volume with this size is created on the respective infrastructure and mounted into the service pod (check this as this can lead to substantial costs!). 100 MAX_CONTAINER_SIZE The maximum size a ML Lab managed container is allowed to grow in Gigabytes. If you add data to a container's writeable layer - basically a path where no volume is mounted - the container size grows. If not set, a container can theoretically consume all of the host's storage. For Docker-local, ML Lab contains a REST-method (/containers/shutdown-disk-exceeding) that will remove all non-core containers that exceed this limit (in Docker-local mode, currently only workspaces are removed). In Kubernetes mode, the native functionality of ephemeral-storage limit is used (for all non-core pods). Set to '-1' to disable it. 100 LAB_DATA_ROOT Basic mount path where all data is stored. (optional) LAB_DATA_WORKSPACE_ROOT Basic mount path where all workspace data is stored. Overwrites the LAB_DATA_ROOT variable for workspace mount. (optional) LAB_DEBUG If true, ML Lab will expose all ports and print out debug logs. false LAB_SSH_ENABLED Enable ssh jumphost if ML Lab should publish port 22 on startup and start an SSH server. The jumphost functionality can be used so that users can ssh into their own workspace. SSHing into the workspace container itself is not possible. true ALLOW_SELF_REGISTRATIONS If true, ML Lab will allow user self registrations via register dialog or automatically create users if external OIDC authentication is enabled. true WORKSPACE_BACKUP If true, workspaces will be automatically backuped every day to the ML Lab Storage and restored if necessary. false WORKSPACE_IMAGE Docker image used for user workspaces. Should be build on top of the ml-workspace base image. (optional) LAB_MANAGED_KUBERNETES Specifies whether it is running on a managed Kubernetes cluster instance. For more information, please have a look at the Section about Kubernetes managed cluster setup. false LAB_STORAGE_CLASS Only relevant for Kubernetes managed cluster setup. The storage class name that is used by the persistent volume claims for issuing the volumes mounted into the pods of Minio, Mongo, and the Workspaces. lab-storageclass LAB_PVC_MINIO_STORAGE_LIMIT Only relevant for Kubernetes managed cluster setup. It defines the size of the volume created and mounted into the Minio pod in GB. Minio is the storage for uploaded files such as datasets and models. 100 LAB_PVC_MONGO_STORAGE_LIMIT Only relevant for Kubernetes managed cluster setup. It defines the size of the volume created and mounted into the Mongo pod in GB. Mongo contains the data for experiments and users. 5 LAB_IMAGE_REGISTRY The registry prefix from where the images lab-service and lab-model-service are loaded. If you, for example, deploy it in Azure you might not have access to the internal Artifactory registry. Make sure to push the images to the defined registry. If you don't use the one-click model-deployment feature, you don't have to push the lab-model-service image. Default DockerHub registry LAB_EXTERNAL_OIDC_AUTH_URL The authorization endpoint used for external OIDC authentication. The client will be redirected to this page to authenticate with the external authentication provider. For detailed information see: External OIDC Authentication (optional) LAB_EXTERNAL_OIDC_TOKEN_URL The token endpoint used for external OIDC authentication. It will be used by the backend to obtain the OIDC identity token in exchange for an authorization code. For detailed information see: External OIDC Authentication (optional) LAB_EXTERNAL_OIDC_CLIENT_ID The OAuth 2.0 client identifier used for external OIDC authentication. For detailed information see: External OIDC Authentication (optional) LAB_EXTERNAL_OIDC_CLIENT_SECRET The OAuth 2.0 client secret used for external OIDC authentication. For detailed information see: External OIDC Authentication (optional)","title":"Parameters"},{"location":"installation/install-lab/#proxy","text":"If a proxy is required, you can pass the proxy configuration via the http_proxy and no_proxy environment variables. For example: --env http_proxy=http://myproxy:1234","title":"Proxy"},{"location":"installation/install-lab/#install-with-docker-local-mode","text":"Note: The install commands here are currently based on the scenario where you built the Docker images locally yourself. For seeing how to use ready images, please check the Readme of the GitHub repository here To start ML Lab in a single-host (local) deployment execute: docker run --rm --env LAB_PORT = 8091 -v /var/run/docker.sock:/var/run/docker.sock --env LAB_ACTION = install lab-service:latest After the installation is finished (after several minutes depending on internet speed), visit http://<HOSTIP>:8091 and login with admin:admin . Enable SSL For SSL setup, create the certificate (files must be called cert.crt and cert.key) and specify their path on the host machine via the LAB_SSL_ROOT environment variable. Additionally you need to set SERVICE_SSL_ENABLED to true: docker run --rm --env LAB_PORT = 8091 \\ --env SERVICE_SSL_ENABLED = true \\ --env LAB_SSL_ROOT = /workspace/ssl \\ -v /var/run/docker.sock:/var/run/docker.sock \\ lab-service:latest Alternatively, instead of specifying LAB_SSL_ROOT , the certificate can be provided in a docker volume named lab_ssl . If you don't provide a custom certificate, a self-signed certificate is generated and used. Be aware that applications such as your browser might not trust the certificate.","title":"Install with Docker Local Mode"},{"location":"installation/install-lab/#install-with-kubernetes-mode-own-cluster","text":"These steps are for a custom cluster that does not use a hyperscaler / automated infrastructure in the background. If you have a managed cluster (such as AWS EKS), then please follow the steps in the next Section. Install Kubernetes For Kubernetes Mode, please make sure that your server/cluster is installed with Docker and Kubernetes (Version >=1.11). You can find more information about Kubernetes installation here . Preparation For the Kubernetes deployment, a few minor steps to prepare the host have to be made such as creating a directory where the data is stored etc. since Kubernetes does not have the concept of Docker's named volumes. The Kubernetes version of the ML Lab needs the kube config of the cluster as well as the access to the Docker socket. # label the master node with role=master, as we use it in ML Lab kubectl label nodes <name-of-master-node> role = master # install nfs-common for mounting nfs-service in workspace apt-get install nfs-common # create the data root directory that will be used by Kubernetes # Default is `/workspace/lab/<namespace>/data` # Hence, when creating a Lab for default namespace 'lab': mkdir -p /workspace/lab/lab/data Installation # On Mac: in ~/.kube/config for 'server' field replace 'localhost' with 'docker.for.mac.localhost' docker run --rm --env LAB_PORT = 30001 \\ -v /root/.kube/config:/root/.kube/config \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --env SERVICES_RUNTIME = k8s \\ --env LAB_DATA_ROOT = /workspace/lab/stulabdio/data \\ lab-service:latest After installation is finished (after several minutes depending on intranet speed), visit http://<HOSTIP>:30001 and login with admin:admin . Tip When a container, e.g. the Workspace container, is launched on a node the first time, it's Docker image has to be pulled to that node. This can take some time in which the user sees a loading screen. To prevent this, you can pull the images manually for each node beforehand. Enable SSL For SSL setup, create the certificates and mount them into the container's directory at /resources/ssl ( -v /workspace/ssl:/resources/ssl:ro ) and start with --env SERVICE_SSL_ENABLED=true : docker run --rm --env LAB_PORT = 30001 \\ -v /root/.kube/config:/root/.kube/config \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --env SERVICES_RUNTIME = k8s \\ --env LAB_DATA_ROOT = /workspace/lab/lab/data \\ --env SERVICE_SSL_ENABLED = true \\ -v /workspace/ssl:/resources/ssl:ro \\ lab-service:latest The files have to be named cert.crt and cert.key .","title":"Install with Kubernetes Mode (Own Cluster)"},{"location":"installation/install-lab/#install-with-kubernetes-managed-cluster-such-as-aws-eks","text":"TODO","title":"Install with Kubernetes (Managed Cluster such as AWS EKS)"},{"location":"installation/install-lab/#difference-to-custom-cluster-setup","text":"In the Managed Cluster scenario, the biggest difference is that you don't have to take care of the volumes as here we leverage persistent volume (claims) to automatically issue volumes in the background. Hence, no need for the NFS service here. In this setup mode, ML Lab accesses the cluster via its ServiceAccount permissions and not via a mounted kube-config. Costs As volumes are created automatically on the infrastructure based on the Kubernetes persistent volume claims, make sure to have an eye on your costs and set the size for the volume via the respective environment variables accordingly. Even when deleting the Kuberentes PersistentVolume and PersistentVolumeClaim resources, the actual volumes might still exist on the cloud provider and have to be deleted manually.","title":"Difference to Custom Cluster Setup"},{"location":"installation/install-lab/#after-installation","text":"Please makes sure to change the admin password ( User-menu -> Change Password ) after the installation was successful. We also recommend to activate a data backup job as described here . Check out the administration section in this documentation for more information on how to update / uninstall Lab, or manage services .","title":"After Installation"},{"location":"usage/core-concepts/","text":"Core Concepts \u00b6 Project \u00b6 A project is a digital space for tackling a specific data science use-case. It consists of multiple datasets, experiments, models, services, and jobs. Dataset \u00b6 A dataset is a single file that contains raw- or processed-data, usually preselected for a certain experiment. It is recommended to have the dataset in an easily readable format such as CSV for structured data or GraphML for graph data. If your dataset consists of multiple files (e.g. collection of images), we recommend packaging (e.g. via zip) this dataset to a single file. Our python libraries have built-in support for his type of packaged data. Experiment \u00b6 An experiment is a single execution of data science code with specific parameters, data, and results. An experiment in the data science field usually refers to a single model training run, but can also be any other computational task, such as a data transformation run, that requires a certain configuration and has some measurable results. All metadata and artifacts for an experiment such as parameters, timestamp, operator, data & model files, environment information, resulting metrics (e.g. accuracy), and other related information can easily be saved and shared with the experiment to enable reproducibility. During the experimentation process, one or more parameters might be changed by the data scientist in an organized manner, and the effects of these changes on associated metrics should be measured, recorded, and analyzed. Data scientist may try many different approaches, different parameters and fail many times before the required level of a metric is achieved. Model \u00b6 A model is an artifact that is created during the training process. A model is something that predicts an output for a given input. Any file created after training from an ML algorithm is a model. The model needs to be deployed as a service in order to offer the model's prediction capabilities for integration into applications. Service \u00b6 A service is a software component that implements and provides specific capabilities. In our landscape, services are deployed as Docker containers and are usually accessible via a REST API over HTTP(S). Job \u00b6 A job is a software component that runs a finite task for a certain time until completion. Jobs are useful for running experiments, such as model training and data pipelines, or other kinds of large computations and batch-oriented tasks. Besides single execution, a job can also be scheduled for automatic execution based on dependencies to other jobs or run periodically via CRON commands. In our landscape, a job is implemented as a Docker container that is expected to run a finite task (script) that returns a successful or failed exit code after completion. We suggest migrating data processing or model training scripts from the workspace into jobs, once the scripts are ready for production usage. This makes it simple to rerun and schedule your production-ready data pipelines and model trainings. Workspace \u00b6 A place for a data scientist to conduct a set of experiments centered around a given project or problem to solve. It can have many different notebooks and scripts containing different experiments that are carried out as the data scientist seek the best model. The workspace is implemented as an all-in-one web IDE specialized for machine learning and data science.","title":"Core Concepts"},{"location":"usage/core-concepts/#core-concepts","text":"","title":"Core Concepts"},{"location":"usage/core-concepts/#project","text":"A project is a digital space for tackling a specific data science use-case. It consists of multiple datasets, experiments, models, services, and jobs.","title":"Project"},{"location":"usage/core-concepts/#dataset","text":"A dataset is a single file that contains raw- or processed-data, usually preselected for a certain experiment. It is recommended to have the dataset in an easily readable format such as CSV for structured data or GraphML for graph data. If your dataset consists of multiple files (e.g. collection of images), we recommend packaging (e.g. via zip) this dataset to a single file. Our python libraries have built-in support for his type of packaged data.","title":"Dataset"},{"location":"usage/core-concepts/#experiment","text":"An experiment is a single execution of data science code with specific parameters, data, and results. An experiment in the data science field usually refers to a single model training run, but can also be any other computational task, such as a data transformation run, that requires a certain configuration and has some measurable results. All metadata and artifacts for an experiment such as parameters, timestamp, operator, data & model files, environment information, resulting metrics (e.g. accuracy), and other related information can easily be saved and shared with the experiment to enable reproducibility. During the experimentation process, one or more parameters might be changed by the data scientist in an organized manner, and the effects of these changes on associated metrics should be measured, recorded, and analyzed. Data scientist may try many different approaches, different parameters and fail many times before the required level of a metric is achieved.","title":"Experiment"},{"location":"usage/core-concepts/#model","text":"A model is an artifact that is created during the training process. A model is something that predicts an output for a given input. Any file created after training from an ML algorithm is a model. The model needs to be deployed as a service in order to offer the model's prediction capabilities for integration into applications.","title":"Model"},{"location":"usage/core-concepts/#service","text":"A service is a software component that implements and provides specific capabilities. In our landscape, services are deployed as Docker containers and are usually accessible via a REST API over HTTP(S).","title":"Service"},{"location":"usage/core-concepts/#job","text":"A job is a software component that runs a finite task for a certain time until completion. Jobs are useful for running experiments, such as model training and data pipelines, or other kinds of large computations and batch-oriented tasks. Besides single execution, a job can also be scheduled for automatic execution based on dependencies to other jobs or run periodically via CRON commands. In our landscape, a job is implemented as a Docker container that is expected to run a finite task (script) that returns a successful or failed exit code after completion. We suggest migrating data processing or model training scripts from the workspace into jobs, once the scripts are ready for production usage. This makes it simple to rerun and schedule your production-ready data pipelines and model trainings.","title":"Job"},{"location":"usage/core-concepts/#workspace","text":"A place for a data scientist to conduct a set of experiments centered around a given project or problem to solve. It can have many different notebooks and scripts containing different experiments that are carried out as the data scientist seek the best model. The workspace is implemented as an all-in-one web IDE specialized for machine learning and data science.","title":"Workspace"},{"location":"usage/experiments-dashboard/","text":"Experiments Dashboard \u00b6 The purpose of the Experiment Dashboard is to compare different approaches, such as training different models or hyperparameter tuning based on metrics you can define in your python code. You are responsible for selecting the metric suitable for your challenge you want to tackle, for example: Accuracy, Recall, F1-measurement. Besides the main comparison metric, you can store arbitrary metrics as well as other information, such as timestamp, operator, data & model files, environment information. You can then use the dashboard to compare the information belonging to experiments. Overview \u00b6 Above the dashboard you can see meta information of all experiments that have run. An experiment itself is a row and can have attributes that are shown as columns. Each experiment has a status, indicating whether the experiment has completed, failed, or is still running. There are also other status such as dead, queued, and interrupted. Those can occur by the nature of Jupyter notebook kernels. Expendable Rows \u00b6 To get an idea of what you can track, you can expand the row to have a more consolidated view on different information types. Not only can you find all the defined metrics for your experiment but also dependencies, used artifacts, host information (such as CPU, OS, number of cores, python version, workspace version and number of GPUs), Git information and all defined parameters and files that you can download directly. To improve reproducibility, you can store your datasets you used for training and testing your model here, so that others easily can take the same dataset you used. Grouping \u00b6 Another handy feature of the Experiment Dashboard is the grouping of attributes. You can easily group attributes, resulting in a tree view. This makes it easy to group experiments by operators of the project, status, or parameters. Additionally, it is possible to combine attributes leading to a nested tree view. Search \u00b6 Within the Dashboard you have two options to filter for specific values. Either you use the overall search for any string matching each column, as shown in the gif below or you use filters dedicated for a specific column. The latter comes with the advantage that you can use comparative expressions. By clicking on the icon filter of the result column, you can choose operands such as greater than, greater, less and so on. By now, there are custom filters for date, text, and numbers. Attribute Selection \u00b6 By default you are looking at the Dashboard with a basic set of columns. Those were curated by us as we think they give a good overview at the first glance. In case you want to compare other attributes as well, like parameters or metrics, you can do so by choosing them after clicking on the Columns button . We gathered all attributes based on the experiments you have run so far at this point, meaning there will only attributes show up you used once. Delete Experiments \u00b6 Sometimes things do not go as expected, e.g. the kernel died because of a memory leak or you had to interrupt the experiment. This might add up and lead to a bunch of useless entries and a messy history. You can delete experiments by clicking on the button at the very right end of the failed experiment.","title":"Experiments Dashboard"},{"location":"usage/experiments-dashboard/#experiments-dashboard","text":"The purpose of the Experiment Dashboard is to compare different approaches, such as training different models or hyperparameter tuning based on metrics you can define in your python code. You are responsible for selecting the metric suitable for your challenge you want to tackle, for example: Accuracy, Recall, F1-measurement. Besides the main comparison metric, you can store arbitrary metrics as well as other information, such as timestamp, operator, data & model files, environment information. You can then use the dashboard to compare the information belonging to experiments.","title":"Experiments Dashboard"},{"location":"usage/experiments-dashboard/#overview","text":"Above the dashboard you can see meta information of all experiments that have run. An experiment itself is a row and can have attributes that are shown as columns. Each experiment has a status, indicating whether the experiment has completed, failed, or is still running. There are also other status such as dead, queued, and interrupted. Those can occur by the nature of Jupyter notebook kernels.","title":"Overview"},{"location":"usage/experiments-dashboard/#expendable-rows","text":"To get an idea of what you can track, you can expand the row to have a more consolidated view on different information types. Not only can you find all the defined metrics for your experiment but also dependencies, used artifacts, host information (such as CPU, OS, number of cores, python version, workspace version and number of GPUs), Git information and all defined parameters and files that you can download directly. To improve reproducibility, you can store your datasets you used for training and testing your model here, so that others easily can take the same dataset you used.","title":"Expendable Rows"},{"location":"usage/experiments-dashboard/#grouping","text":"Another handy feature of the Experiment Dashboard is the grouping of attributes. You can easily group attributes, resulting in a tree view. This makes it easy to group experiments by operators of the project, status, or parameters. Additionally, it is possible to combine attributes leading to a nested tree view.","title":"Grouping"},{"location":"usage/experiments-dashboard/#search","text":"Within the Dashboard you have two options to filter for specific values. Either you use the overall search for any string matching each column, as shown in the gif below or you use filters dedicated for a specific column. The latter comes with the advantage that you can use comparative expressions. By clicking on the icon filter of the result column, you can choose operands such as greater than, greater, less and so on. By now, there are custom filters for date, text, and numbers.","title":"Search"},{"location":"usage/experiments-dashboard/#attribute-selection","text":"By default you are looking at the Dashboard with a basic set of columns. Those were curated by us as we think they give a good overview at the first glance. In case you want to compare other attributes as well, like parameters or metrics, you can do so by choosing them after clicking on the Columns button . We gathered all attributes based on the experiments you have run so far at this point, meaning there will only attributes show up you used once.","title":"Attribute Selection"},{"location":"usage/experiments-dashboard/#delete-experiments","text":"Sometimes things do not go as expected, e.g. the kernel died because of a memory leak or you had to interrupt the experiment. This might add up and lead to a bunch of useless entries and a messy history. You can delete experiments by clicking on the button at the very right end of the failed experiment.","title":"Delete Experiments"},{"location":"usage/how-to-create-a-job/","text":"How to create a Job \u00b6 A job is a software component that runs a finite task for a certain time to completion. Jobs are useful for running experiments, such as model training and data pipelines, or other kinds of large computations and batch-oriented tasks. We suggest migrating data processing or model training scripts from the workspace into jobs, once the scripts are ready for production usage. This makes it simple to rerun and schedule your production-ready data pipelines and model trainings. Minimum Requirements \u00b6 A job is required to be implemented as a Docker container that is expected to run a finite task that returns a successful or failed exit code after completion. Furthermore, we advise that the job image adheres common docker standards and best practices . Status Codes Your job should return the right status code once it is finished. A status code of 0 means the job succeeded . A status code different to 0 means, the job failed. Since we only require the job to be a Docker container, there are no technological restrictions on how the task is implemented (e.g. python/bash/java script). Default Configuration \u00b6 If a connection to an ML Lab instance is required, the job should make use of the following set of environment variables that are automatically passed to the job if started from ML Lab: Variable Description Default LAB_ENDPOINT Endpoint URL of an ML Lab instance. (optional) LAB_API_TOKEN API Token to access the REST API of an ML Lab instance. (optional) LAB_PROJECT Specified project of an ML Lab Instance. (optional) The lab python client and the lab java client will automatically use those environment variables to initialize a connection to Lab. Best Practices \u00b6 We have a few best practices which we recommend to apply for developing your jobs: Always specify container parameters with default values directly inside the Dockerfile ( ENV EXAMPLE=example ) Prefix internal environment variables that should not be changed with an underscore ( _RESOURCE_PATH ) If the container has data that needs to be persisted put them int the /data folder (this is the default folder to be mounted). Use default stdout for logging as explained in the Docker documenation . If possible, use Ubuntu as base image. Job Deployment \u00b6 To be able to deploy a job to Lab or any other Docker/Kubernetes infrastructure, the job image needs to be pushed to an accessible Docker registry such as Docker Hub . ML Lab \u00b6 The easiest way to run a job in an ML Lab instance is via the Jobs UI by providing the job image and, optionally, a name and parameters (environment variables): Besides single-run execution, a job can also be scheduled using the UNIX CRON definition . A scheduled job will run at approximately the given time, with a possible delay of seconds/minutes. As an alternative, you can also use the POST /lab/projects/{project}/jobs REST API method of Lab to programmatically run or schedule a job. Additionaly, you can also use the Python client to deploy a job to any Lab instance as shown below: from lab_client import Environment env = Environment ( project = \"<LAB_PROJECT>\" , lab_endpoint = \"<LAB_ENDPOINT_URL>\" , lab_api_token = \"<LAB_API_TOKEN>\" ) from lab_client.handler.lab_job_handler import LabJobConfig , LabJobHandler job_handler = LabJobHandler ( env ) # Configure the job demo_job = LabJobConfig ( image = \"<YOUR_JOB_DOCKER_IMAGE>\" , name = \"my-job\" , params = { \"EXAMPLE_PARAMETER\" : \"example\" } ) # Run the job. To run the job in background, set check_status to False. job_handler . run_job ( demo_job , check_status = True ) Docker Infrastructure \u00b6 Since the job is just a normal Docker container, you can run it on any Docker infrastructure via docker run and provide the required parameters via --env : docker run -d --env EXAMPLE_PARAMETER = example <YOUR_JOB_DOCKER_IMAGE> Kubernetes Infrastructure \u00b6 For Kubernetes, please refer to this guide . Run job via workspace image \u00b6 The workspace image can also be used to execute arbitrary Python code without starting any of the preinstalled tools. This provides a seamless way to productize your ML projects since the code that has been developed interactively within the workspace will have the same environment and configuration when run as a job via the same workspace image. To run Python code as a job, you need to provide a path or URL to a code directory (or script) via EXECUTE_CODE . The code can be either already mounted into the workspace container or downloaded from a version control system (e.g., git or svn) as described in the following sections. The selected code path needs to be python executable. In case the selected code is a directory (e.g., whenever you download the code from a VCS) you need to put a __main__.py file at the root of this directory. The __main__.py needs to contain the code that starts your job. Run code from version control system \u00b6 You can execute code directly from Git, Mercurial, Subversion, or Bazaar by using the pip-vcs format as described in this guide . For example, to execute code from a subdirectory of a git repository, just run: docker run --env EXECUTE_CODE = \"git+https://github.com/ml-tooling/ml-workspace.git#subdirectory=docker-res/tests/ml-job\" mltooling/ml-workspace:latest Select branches, commits, or tags For additional information on how to specify branches, commits, or tags please refer to this guide . Run code mounted into workspace \u00b6 In the following example, we mount and execute the current working directory (expected to contain our code) into the /workspace/ml-job/ directory of the workspace: docker run -v \" ${ PWD } :/workspace/ml-job/\" --env EXECUTE_CODE = \"/workspace/ml-job/\" mltooling/ml-workspace:latest Run code from Lab data \u00b6 You can also run the job via data uploaded to Lab (single script or packaged code) by specifing the key via the EXECUTE_CODE variable. Install Dependencies \u00b6 In the case that the preinstalled workspace libraries are not compatible with your code, you can install or change dependencies by just adding one or multiple of the following files to your code directory: requirements.txt : pip requirements format for pip-installable dependencies. environment.yml : conda environment file to create a separate Python environment. setup.sh : A shell script executed via /bin/bash . The execution order is 1. environment.yml -> 2. setup.sh -> 3. requirements.txt Test job in interactive mode \u00b6 You can test your job code within the workspace (started normally with interactive tools) by executing the following python script: python /resources/scripts/execute_code.py /path/to/your/job Build a custom job image \u00b6 It is also possible to embed your code directly into a custom job image, as shown below: FROM mltooling/ml-workspace:latest # Add job code to image COPY ml-job /workspace/ml-job ENV EXECUTE_CODE = /workspace/ml-job # Install requirements only RUN python /resources/scripts/execute_code.py --requirements-only # Execute only the code at container startup CMD [ \"python\" , \"/resources/docker-entrypoint.py\" , \"--code-only\" ]","title":"How to create a Job"},{"location":"usage/how-to-create-a-job/#how-to-create-a-job","text":"A job is a software component that runs a finite task for a certain time to completion. Jobs are useful for running experiments, such as model training and data pipelines, or other kinds of large computations and batch-oriented tasks. We suggest migrating data processing or model training scripts from the workspace into jobs, once the scripts are ready for production usage. This makes it simple to rerun and schedule your production-ready data pipelines and model trainings.","title":"How to create a Job"},{"location":"usage/how-to-create-a-job/#minimum-requirements","text":"A job is required to be implemented as a Docker container that is expected to run a finite task that returns a successful or failed exit code after completion. Furthermore, we advise that the job image adheres common docker standards and best practices . Status Codes Your job should return the right status code once it is finished. A status code of 0 means the job succeeded . A status code different to 0 means, the job failed. Since we only require the job to be a Docker container, there are no technological restrictions on how the task is implemented (e.g. python/bash/java script).","title":"Minimum Requirements"},{"location":"usage/how-to-create-a-job/#default-configuration","text":"If a connection to an ML Lab instance is required, the job should make use of the following set of environment variables that are automatically passed to the job if started from ML Lab: Variable Description Default LAB_ENDPOINT Endpoint URL of an ML Lab instance. (optional) LAB_API_TOKEN API Token to access the REST API of an ML Lab instance. (optional) LAB_PROJECT Specified project of an ML Lab Instance. (optional) The lab python client and the lab java client will automatically use those environment variables to initialize a connection to Lab.","title":"Default Configuration"},{"location":"usage/how-to-create-a-job/#best-practices","text":"We have a few best practices which we recommend to apply for developing your jobs: Always specify container parameters with default values directly inside the Dockerfile ( ENV EXAMPLE=example ) Prefix internal environment variables that should not be changed with an underscore ( _RESOURCE_PATH ) If the container has data that needs to be persisted put them int the /data folder (this is the default folder to be mounted). Use default stdout for logging as explained in the Docker documenation . If possible, use Ubuntu as base image.","title":"Best Practices"},{"location":"usage/how-to-create-a-job/#job-deployment","text":"To be able to deploy a job to Lab or any other Docker/Kubernetes infrastructure, the job image needs to be pushed to an accessible Docker registry such as Docker Hub .","title":"Job Deployment"},{"location":"usage/how-to-create-a-job/#ml-lab","text":"The easiest way to run a job in an ML Lab instance is via the Jobs UI by providing the job image and, optionally, a name and parameters (environment variables): Besides single-run execution, a job can also be scheduled using the UNIX CRON definition . A scheduled job will run at approximately the given time, with a possible delay of seconds/minutes. As an alternative, you can also use the POST /lab/projects/{project}/jobs REST API method of Lab to programmatically run or schedule a job. Additionaly, you can also use the Python client to deploy a job to any Lab instance as shown below: from lab_client import Environment env = Environment ( project = \"<LAB_PROJECT>\" , lab_endpoint = \"<LAB_ENDPOINT_URL>\" , lab_api_token = \"<LAB_API_TOKEN>\" ) from lab_client.handler.lab_job_handler import LabJobConfig , LabJobHandler job_handler = LabJobHandler ( env ) # Configure the job demo_job = LabJobConfig ( image = \"<YOUR_JOB_DOCKER_IMAGE>\" , name = \"my-job\" , params = { \"EXAMPLE_PARAMETER\" : \"example\" } ) # Run the job. To run the job in background, set check_status to False. job_handler . run_job ( demo_job , check_status = True )","title":"ML Lab"},{"location":"usage/how-to-create-a-job/#docker-infrastructure","text":"Since the job is just a normal Docker container, you can run it on any Docker infrastructure via docker run and provide the required parameters via --env : docker run -d --env EXAMPLE_PARAMETER = example <YOUR_JOB_DOCKER_IMAGE>","title":"Docker Infrastructure"},{"location":"usage/how-to-create-a-job/#kubernetes-infrastructure","text":"For Kubernetes, please refer to this guide .","title":"Kubernetes Infrastructure"},{"location":"usage/how-to-create-a-job/#run-job-via-workspace-image","text":"The workspace image can also be used to execute arbitrary Python code without starting any of the preinstalled tools. This provides a seamless way to productize your ML projects since the code that has been developed interactively within the workspace will have the same environment and configuration when run as a job via the same workspace image. To run Python code as a job, you need to provide a path or URL to a code directory (or script) via EXECUTE_CODE . The code can be either already mounted into the workspace container or downloaded from a version control system (e.g., git or svn) as described in the following sections. The selected code path needs to be python executable. In case the selected code is a directory (e.g., whenever you download the code from a VCS) you need to put a __main__.py file at the root of this directory. The __main__.py needs to contain the code that starts your job.","title":"Run job via workspace image"},{"location":"usage/how-to-create-a-job/#run-code-from-version-control-system","text":"You can execute code directly from Git, Mercurial, Subversion, or Bazaar by using the pip-vcs format as described in this guide . For example, to execute code from a subdirectory of a git repository, just run: docker run --env EXECUTE_CODE = \"git+https://github.com/ml-tooling/ml-workspace.git#subdirectory=docker-res/tests/ml-job\" mltooling/ml-workspace:latest Select branches, commits, or tags For additional information on how to specify branches, commits, or tags please refer to this guide .","title":"Run code from version control system"},{"location":"usage/how-to-create-a-job/#run-code-mounted-into-workspace","text":"In the following example, we mount and execute the current working directory (expected to contain our code) into the /workspace/ml-job/ directory of the workspace: docker run -v \" ${ PWD } :/workspace/ml-job/\" --env EXECUTE_CODE = \"/workspace/ml-job/\" mltooling/ml-workspace:latest","title":"Run code mounted into workspace"},{"location":"usage/how-to-create-a-job/#run-code-from-lab-data","text":"You can also run the job via data uploaded to Lab (single script or packaged code) by specifing the key via the EXECUTE_CODE variable.","title":"Run code from Lab data"},{"location":"usage/how-to-create-a-job/#install-dependencies","text":"In the case that the preinstalled workspace libraries are not compatible with your code, you can install or change dependencies by just adding one or multiple of the following files to your code directory: requirements.txt : pip requirements format for pip-installable dependencies. environment.yml : conda environment file to create a separate Python environment. setup.sh : A shell script executed via /bin/bash . The execution order is 1. environment.yml -> 2. setup.sh -> 3. requirements.txt","title":"Install Dependencies"},{"location":"usage/how-to-create-a-job/#test-job-in-interactive-mode","text":"You can test your job code within the workspace (started normally with interactive tools) by executing the following python script: python /resources/scripts/execute_code.py /path/to/your/job","title":"Test job in interactive mode"},{"location":"usage/how-to-create-a-job/#build-a-custom-job-image","text":"It is also possible to embed your code directly into a custom job image, as shown below: FROM mltooling/ml-workspace:latest # Add job code to image COPY ml-job /workspace/ml-job ENV EXECUTE_CODE = /workspace/ml-job # Install requirements only RUN python /resources/scripts/execute_code.py --requirements-only # Execute only the code at container startup CMD [ \"python\" , \"/resources/docker-entrypoint.py\" , \"--code-only\" ]","title":"Build a custom job image"},{"location":"usage/how-to-create-a-service/","text":"How to create a Service \u00b6 A service is a software component that implements and provides specific capabilities. Usually, a service provides REST APIs over HTTP(S) but can also provide other types of protocols that can be served through a TCP port. In the context of machine learning, a service is usually used to make trained models accessible for integration into applications. If your goal is to deploy a model that was trained with a Python framework, the easiest way is to use the Unified Model Library . If you want to build your own custom ML Lab compatible service, you are at least required to fulfill a small set of minimum requirements . Minimum Requirements \u00b6 A service is required to be implemented as a Docker container that is expected to have at least one HTTP server exposed on any given port. Furthermore, we advise that the service image adheres common docker standards and best practices . Expose Ports All ports that need to be exposed are required to be mentioned via the EXPOSE instruction inside the Dockerfile. Since we only require the service to be a Docker container, there are no technological restrictions on how the service is implemented (e.g. python/bash/java server). A service deployed in an ML Lab project is only accessible from a relative path (as explained below) and requires a valid API token (via Authorization header, cookie, or get-parameter) to be accessed. You can get the API token in the user menu (top-right corner -> Get Project API Token ). Default Configuration \u00b6 If a connection to an ML Lab instance is required, the service should make use of the following set of environment variables that are automatically passed to the service if started from within the web app: Variable Description Default LAB_ENDPOINT Endpoint URL of an ML Lab instance. (optional) LAB_API_TOKEN API Token to access the REST API of an ML Lab instance. (optional) LAB_PROJECT Specified project of an ML Lab Instance. (optional) LAB_SERVICE_PATH Base URL path to reach the service within ML Lab. The container internal port for the given endpoint needs to be added to this path as well. (optional) The lab python client and the lab java client will automatically use those environment variables to initialize a connection to ML Lab. Best Practices \u00b6 We have a few best practices which we recommend to apply for developing your service: Always specify container parameters with default values directly inside the Dockerfile ( ENV EXAMPLE=example ) Prefix internal environment variables that should not be changed with an underscore ( _RESOURCE_PATH ) Use the EXPOSE instruction in the Dockerfile to make all required ports of your service available. If the container has data that needs to be persisted put them int the /data folder (this is the default folder to be mounted). If there is a way to check the health status of your service, implement the HEALTHCHECK instruction in the Dockerfile. Use default stdout for logging as explained in the Docker documenation . If possible, use Ubuntu as base image. Example Services \u00b6 We prepared a few simple examples of valid Docker images for services with different Python web frameworks: Fastapi Example : TODO Flask Example : TODO Flask-RESTPlus Example : TODO We recommend to use Fastapi for building your REST APIs. Service Deployment \u00b6 To be able to deploy a service to ML Lab or any other Docker/Kubernetes infrastructure, the service image needs to be pushed to an accessible Docker registry such as Docker Hub . ML Lab \u00b6 The easiest way to deploy a service in an ML Lab instance is via the Services UI by providing the service image and, optionally, a name and parameters (environment variables): As an alternative, you can also use the POST /api/projects/{project}/services REST API method of ML Lab to programmatically deploy a service. Additionaly, you can also use the Python client to deploy a service to any ML Lab instance as shown below: from lab_client import Environment env = Environment ( project = \"<LAB_PROJECT>\" , lab_endpoint = \"<LAB_ENDPOINT_URL>\" , lab_api_token = \"<LAB_API_TOKEN>\" ) # Deploy a Service env . lab_handler . lab_api . deploy_service ( project = env . project , image = \"<YOUR_SERVICE_DOCKER_IMAGE>\" , name = \"my-service\" , # Provide service configuration in request body body = { \"EXAMPLE_PARAMETER\" : \"example\" }) Once the service is deployed, it is accessible from the following URL path: /api/projects/<PROJECT>/services/<SERVICE_NAME>/<PORT>/ . This means that a service can only be used when it is deployed via ML Lab if it is able to serve its content from a relativ path. Furthermore, every so deployed service is secured. Therefore, a valid ML Lab API token is required to access the service via the mentioned URL. Please provide a valid user or project API token via the Authorization header in this format: Bearer YOUR_API_TOKEN . You can get your API token in the user menu (top-right corner) of Lab. As an alternative to the Authorization header, you can also provide the API token via cookie ( lab_access_token ) or via get paramter in the URL ( lab_token ). API token via URL parameter Setting the API token via URL parameter will set the lab_access_token at the first request and, thereby, logs the user out, in case the user accessing the link is logged into the same ML Lab instance. Docker Infrastructure \u00b6 Since the service is just a normal Docker container, you can run it on any Docker infrastructure via docker run by exposing all marked ports via -P . Required parameters (configuration) can be provided via --env : docker run -d -P --env EXAMPLE_PARAMETER = example <YOUR_SERVICE_DOCKER_IMAGE> Kubernetes Infrastructure \u00b6 For Kubernetes, please refer to this guide . Run service via workspace image \u00b6 The workspace image can also be used to execute arbitrary Python code without starting any of the preinstalled tools. This provides a seamless way to productize your ML projects since the code that has been developed interactively within the workspace will have the same environment and configuration when run as a service via the same workspace image. To run Python code as a service, you need to provide a path or URL to a code directory (or script) via EXECUTE_CODE . The code can be either already mounted into the workspace container or downloaded from a version control system (e.g., git or svn) as described in the following sections. The selected code path needs to be python executable. In case the selected code is a directory (e.g., whenever you download the code from a VCS) you need to put a __main__.py file at the root of this directory. The __main__.py needs to contain the code that starts your service. Server on port 8091 The code is expected to start a server on port 8091, since a service requires a running HTTP server. Run code from version control system \u00b6 You can execute code directly from Git, Mercurial, Subversion, or Bazaar by using the pip-vcs format as described in this guide . For example, to execute code from a subdirectory of a git repository, just run: docker run -p 8091 :8091 --env EXECUTE_CODE = \"git+https://github.com/ml-tooling/ml-workspace.git#subdirectory=docker-res/tests/ml-service\" mltooling/ml-workspace:latest Select branches, commits, or tags For additional information on how to specify branches, commits, or tags please refer to this guide . Run code mounted into workspace \u00b6 In the following example, we mount and execute the current working directory (expected to contain our code) into the /workspace/ml-service/ directory of the workspace: docker run -p 8091 :8091 -v \" ${ PWD } :/workspace/ml-service/\" --env EXECUTE_CODE = \"/workspace/ml-service/\" mltooling/ml-workspace:latest Run code from ML Lab data \u00b6 You can also run the service via data uploaded to ML Lab (single script or packaged code) by specifing the key via the EXECUTE_CODE variable. Install Dependencies \u00b6 In the case that the preinstalled workspace libraries are not compatible with your code, you can install or change dependencies by just adding one or multiple of the following files to your code directory: requirements.txt : pip requirements format for pip-installable dependencies. environment.yml : conda environment file to create a separate Python environment. setup.sh : A shell script executed via /bin/bash . The execution order is 1. environment.yml -> 2. setup.sh -> 3. requirements.txt Test service in interactive mode \u00b6 You can test your service code within the workspace (started normally with interactive tools) by executing the following python script: python /resources/scripts/execute_code.py /path/to/your/service Build a custom service image \u00b6 It is also possible to embed your code directly into a custom service image, as shown below: FROM mltooling/ml-workspace:latest # Add service code to image COPY ml-service /workspace/ml-service ENV EXECUTE_CODE = /workspace/ml-service # Install requirements only RUN python /resources/scripts/execute_code.py --requirements-only # Execute only the code at container startup CMD [ \"python\" , \"/resources/docker-entrypoint.py\" , \"--code-only\" ]","title":"How to create a Service"},{"location":"usage/how-to-create-a-service/#how-to-create-a-service","text":"A service is a software component that implements and provides specific capabilities. Usually, a service provides REST APIs over HTTP(S) but can also provide other types of protocols that can be served through a TCP port. In the context of machine learning, a service is usually used to make trained models accessible for integration into applications. If your goal is to deploy a model that was trained with a Python framework, the easiest way is to use the Unified Model Library . If you want to build your own custom ML Lab compatible service, you are at least required to fulfill a small set of minimum requirements .","title":"How to create a Service"},{"location":"usage/how-to-create-a-service/#minimum-requirements","text":"A service is required to be implemented as a Docker container that is expected to have at least one HTTP server exposed on any given port. Furthermore, we advise that the service image adheres common docker standards and best practices . Expose Ports All ports that need to be exposed are required to be mentioned via the EXPOSE instruction inside the Dockerfile. Since we only require the service to be a Docker container, there are no technological restrictions on how the service is implemented (e.g. python/bash/java server). A service deployed in an ML Lab project is only accessible from a relative path (as explained below) and requires a valid API token (via Authorization header, cookie, or get-parameter) to be accessed. You can get the API token in the user menu (top-right corner -> Get Project API Token ).","title":"Minimum Requirements"},{"location":"usage/how-to-create-a-service/#default-configuration","text":"If a connection to an ML Lab instance is required, the service should make use of the following set of environment variables that are automatically passed to the service if started from within the web app: Variable Description Default LAB_ENDPOINT Endpoint URL of an ML Lab instance. (optional) LAB_API_TOKEN API Token to access the REST API of an ML Lab instance. (optional) LAB_PROJECT Specified project of an ML Lab Instance. (optional) LAB_SERVICE_PATH Base URL path to reach the service within ML Lab. The container internal port for the given endpoint needs to be added to this path as well. (optional) The lab python client and the lab java client will automatically use those environment variables to initialize a connection to ML Lab.","title":"Default Configuration"},{"location":"usage/how-to-create-a-service/#best-practices","text":"We have a few best practices which we recommend to apply for developing your service: Always specify container parameters with default values directly inside the Dockerfile ( ENV EXAMPLE=example ) Prefix internal environment variables that should not be changed with an underscore ( _RESOURCE_PATH ) Use the EXPOSE instruction in the Dockerfile to make all required ports of your service available. If the container has data that needs to be persisted put them int the /data folder (this is the default folder to be mounted). If there is a way to check the health status of your service, implement the HEALTHCHECK instruction in the Dockerfile. Use default stdout for logging as explained in the Docker documenation . If possible, use Ubuntu as base image.","title":"Best Practices"},{"location":"usage/how-to-create-a-service/#example-services","text":"We prepared a few simple examples of valid Docker images for services with different Python web frameworks: Fastapi Example : TODO Flask Example : TODO Flask-RESTPlus Example : TODO We recommend to use Fastapi for building your REST APIs.","title":"Example Services"},{"location":"usage/how-to-create-a-service/#service-deployment","text":"To be able to deploy a service to ML Lab or any other Docker/Kubernetes infrastructure, the service image needs to be pushed to an accessible Docker registry such as Docker Hub .","title":"Service Deployment"},{"location":"usage/how-to-create-a-service/#ml-lab","text":"The easiest way to deploy a service in an ML Lab instance is via the Services UI by providing the service image and, optionally, a name and parameters (environment variables): As an alternative, you can also use the POST /api/projects/{project}/services REST API method of ML Lab to programmatically deploy a service. Additionaly, you can also use the Python client to deploy a service to any ML Lab instance as shown below: from lab_client import Environment env = Environment ( project = \"<LAB_PROJECT>\" , lab_endpoint = \"<LAB_ENDPOINT_URL>\" , lab_api_token = \"<LAB_API_TOKEN>\" ) # Deploy a Service env . lab_handler . lab_api . deploy_service ( project = env . project , image = \"<YOUR_SERVICE_DOCKER_IMAGE>\" , name = \"my-service\" , # Provide service configuration in request body body = { \"EXAMPLE_PARAMETER\" : \"example\" }) Once the service is deployed, it is accessible from the following URL path: /api/projects/<PROJECT>/services/<SERVICE_NAME>/<PORT>/ . This means that a service can only be used when it is deployed via ML Lab if it is able to serve its content from a relativ path. Furthermore, every so deployed service is secured. Therefore, a valid ML Lab API token is required to access the service via the mentioned URL. Please provide a valid user or project API token via the Authorization header in this format: Bearer YOUR_API_TOKEN . You can get your API token in the user menu (top-right corner) of Lab. As an alternative to the Authorization header, you can also provide the API token via cookie ( lab_access_token ) or via get paramter in the URL ( lab_token ). API token via URL parameter Setting the API token via URL parameter will set the lab_access_token at the first request and, thereby, logs the user out, in case the user accessing the link is logged into the same ML Lab instance.","title":"ML Lab"},{"location":"usage/how-to-create-a-service/#docker-infrastructure","text":"Since the service is just a normal Docker container, you can run it on any Docker infrastructure via docker run by exposing all marked ports via -P . Required parameters (configuration) can be provided via --env : docker run -d -P --env EXAMPLE_PARAMETER = example <YOUR_SERVICE_DOCKER_IMAGE>","title":"Docker Infrastructure"},{"location":"usage/how-to-create-a-service/#kubernetes-infrastructure","text":"For Kubernetes, please refer to this guide .","title":"Kubernetes Infrastructure"},{"location":"usage/how-to-create-a-service/#run-service-via-workspace-image","text":"The workspace image can also be used to execute arbitrary Python code without starting any of the preinstalled tools. This provides a seamless way to productize your ML projects since the code that has been developed interactively within the workspace will have the same environment and configuration when run as a service via the same workspace image. To run Python code as a service, you need to provide a path or URL to a code directory (or script) via EXECUTE_CODE . The code can be either already mounted into the workspace container or downloaded from a version control system (e.g., git or svn) as described in the following sections. The selected code path needs to be python executable. In case the selected code is a directory (e.g., whenever you download the code from a VCS) you need to put a __main__.py file at the root of this directory. The __main__.py needs to contain the code that starts your service. Server on port 8091 The code is expected to start a server on port 8091, since a service requires a running HTTP server.","title":"Run service via workspace image"},{"location":"usage/how-to-create-a-service/#run-code-from-version-control-system","text":"You can execute code directly from Git, Mercurial, Subversion, or Bazaar by using the pip-vcs format as described in this guide . For example, to execute code from a subdirectory of a git repository, just run: docker run -p 8091 :8091 --env EXECUTE_CODE = \"git+https://github.com/ml-tooling/ml-workspace.git#subdirectory=docker-res/tests/ml-service\" mltooling/ml-workspace:latest Select branches, commits, or tags For additional information on how to specify branches, commits, or tags please refer to this guide .","title":"Run code from version control system"},{"location":"usage/how-to-create-a-service/#run-code-mounted-into-workspace","text":"In the following example, we mount and execute the current working directory (expected to contain our code) into the /workspace/ml-service/ directory of the workspace: docker run -p 8091 :8091 -v \" ${ PWD } :/workspace/ml-service/\" --env EXECUTE_CODE = \"/workspace/ml-service/\" mltooling/ml-workspace:latest","title":"Run code mounted into workspace"},{"location":"usage/how-to-create-a-service/#run-code-from-ml-lab-data","text":"You can also run the service via data uploaded to ML Lab (single script or packaged code) by specifing the key via the EXECUTE_CODE variable.","title":"Run code from ML Lab data"},{"location":"usage/how-to-create-a-service/#install-dependencies","text":"In the case that the preinstalled workspace libraries are not compatible with your code, you can install or change dependencies by just adding one or multiple of the following files to your code directory: requirements.txt : pip requirements format for pip-installable dependencies. environment.yml : conda environment file to create a separate Python environment. setup.sh : A shell script executed via /bin/bash . The execution order is 1. environment.yml -> 2. setup.sh -> 3. requirements.txt","title":"Install Dependencies"},{"location":"usage/how-to-create-a-service/#test-service-in-interactive-mode","text":"You can test your service code within the workspace (started normally with interactive tools) by executing the following python script: python /resources/scripts/execute_code.py /path/to/your/service","title":"Test service in interactive mode"},{"location":"usage/how-to-create-a-service/#build-a-custom-service-image","text":"It is also possible to embed your code directly into a custom service image, as shown below: FROM mltooling/ml-workspace:latest # Add service code to image COPY ml-service /workspace/ml-service ENV EXECUTE_CODE = /workspace/ml-service # Install requirements only RUN python /resources/scripts/execute_code.py --requirements-only # Execute only the code at container startup CMD [ \"python\" , \"/resources/docker-entrypoint.py\" , \"--code-only\" ]","title":"Build a custom service image"},{"location":"usage/lab-api/","text":"Lab REST API \u00b6 You can access the Lab REST API documentation by selecting Lab API Explorer in the menu. The Lab REST API provides all the functionality to create and manage Lab projects, services, datasets, models, experiments, and users. Tip If you are logged into the Lab, your API token will be automatically used when calling a function from the API explorer. The Lab Python Client provides a full python client for this REST API.","title":"ML Lab REST API"},{"location":"usage/lab-api/#lab-rest-api","text":"You can access the Lab REST API documentation by selecting Lab API Explorer in the menu. The Lab REST API provides all the functionality to create and manage Lab projects, services, datasets, models, experiments, and users. Tip If you are logged into the Lab, your API token will be automatically used when calling a function from the API explorer. The Lab Python Client provides a full python client for this REST API.","title":"Lab REST API"},{"location":"usage/lab-python-client/","text":"ML Lab Python Client \u00b6 The python client for ML Lab allows you to connect to any ML Lab instance in order to upload & download files, sync experiments, access ML Lab services, and more. The ML Lab python client is already preinstalled within the research workspace. If you want to use the client outside of the workspace, you can find information on how to install it here . The best way to find out how to use this client library is the lab client tutorial that is also included in the workspace. Client Tutorial \u00b6 The lab client tutorial demonstrates you how to connect to ML Lab and run & manage your experiments within python. We highly recommend to go through this tutorial by yourself. This tutorial is available inside your workspace in the tutorial folder. API Documentation \u00b6 TODO","title":"ML Lab Python Client"},{"location":"usage/lab-python-client/#ml-lab-python-client","text":"The python client for ML Lab allows you to connect to any ML Lab instance in order to upload & download files, sync experiments, access ML Lab services, and more. The ML Lab python client is already preinstalled within the research workspace. If you want to use the client outside of the workspace, you can find information on how to install it here . The best way to find out how to use this client library is the lab client tutorial that is also included in the workspace.","title":"ML Lab Python Client"},{"location":"usage/lab-python-client/#client-tutorial","text":"The lab client tutorial demonstrates you how to connect to ML Lab and run & manage your experiments within python. We highly recommend to go through this tutorial by yourself. This tutorial is available inside your workspace in the tutorial folder.","title":"Client Tutorial"},{"location":"usage/lab-python-client/#api-documentation","text":"TODO","title":"API Documentation"},{"location":"usage/lab-workspace/","text":"Introduction to the Workspace \u00b6 The workspace is an all-in-one web IDE specialized for machine learning and data science. It comes with Jupyter Notebook, a Desktop GUI, Git Integration, Tensorboard Integration, Hardware Monitoring, and many common ML libraries. It is also project-independent, therefore, you will use the same workspace for all of the projects within the given ML Lab instance. The workspace you\u2019re using, including any custom files and libraries that you have setup, will not be accessible by any other user. The workspace is pre-installed with a variety of common data science libraries and features such as Tensorflow, Pytorch, Keras, Sklearn, XGboost, and many more. Jupyter Basics \u00b6 Jupyter Notebook (the application you are currently using) is a web-based interactive environment for writing and running code. The main building blocks of Jupyter are the file-browser, the notebook editor, and kernels. The file-browser provides an interactive file manager for all notebooks, files, and folders in the /workspace directory. A new notebook can be created by clicking on the New drop-down button at the top of the list and selecting the desired language kernel. Terminal You can spawn interactive terminal instances as well by selecting New -> Terminal in the file-browser. The notebook editor enables users to author documents that include live code, markdown text, shell commands, LaTeX equations, interactive widgets, plots, and images. These notebook documents provide a complete and self-contained record of a computation that can be converted to various formats and shared with others. An interactive tour of the notebook UI can be started by selecting Help -> User Interface Tour from the notebook menu bar. If you are a first-time Jupyter user, you can find a more detailed introduction in the Jupyter Basics Tutorial inside your workspace as well. Extensions The workspace has a variety of third-party Jupyter extensions activated. You can configure these extensions in the nbextensions configurator: Edit -> nbextensions config The Notebook allows code to be run in a range of different programming languages. For each notebook document that a user opens, the web application starts a kernel that runs the code for that notebook and returns output. The workspace has a Python 3 and Python 2 kernel pre-installed. Additional Kernels can be installed to get access to other languages (R, Scala, Go, ...) or additional computing resources (GPUs, CPUs, Memory, ...). Python 2 Support Python 2 support is deprecated and not fully supported. Please only use Python 2 if necessary! /workspace Folder \u00b6 The default work directory is /workspace which is also the root directory of the Jupyter instance. Everything in that directory is persisted on the underlying host and backed up to the configured ML Lab instance every day. Data Persistence Use the /workspace directory for all your important work artifacts! Data within other directories might get lost. Install Dependencies \u00b6 Within your workspace you have full root & sudo access to install any library or tool you need by using !pip install or !apt-get install inside the notebook, as shown below: !pip install matplotlib-venn Install Dependencies in Notebook It\u2019s a good idea to include cells which install and load any custom libraries or files (which are not pre-installed in the workspace) that your notebook needs. Connect to ML Lab \u00b6 The workspace has a python lab-client library pre-installed which provides easy access to the ML Lab API and capabilities to download/upload data, and run experiments. Below is a sneak peak into how you can use this library. To be able to execute this code, please replace LAB_PROJECT with one of your projects on the connected ML Lab instance and upload this dataset to the connected project from dataset section of the ML Lab UI. Info To run this code you need to have created a project and uploaded the provided dataset file to the ML Lab instance! from lab_client import Environment import random # Initialize environment env = Environment ( project = \"LAB_PROJECT\" , # ML Lab project you want to work on # Only required in stand-alone workspace deployments # lab_endpoint=\"LAB_ENDPOINT\", # lab_api_token=\"LAB_TOKEN\" ) # Show environment information env . print_info () # Create experiment exp = env . create_experiment ( 'Welcome Tutorial' ) # Get file (make sure that you have uploaded the dataset in the connected project) text_corpus_path = env . get_file ( 'datasets/news-categorized.csv' ) # Define experiment def shuffle_lines ( exp , params , artifacts ): # for this example we will just shuffle all lines in a file random . seed ( params [ \"seed\" ]) lines = open ( text_corpus_path ) . readlines () random . shuffle ( lines ) shuffeled_file_path = exp . create_file_path ( \"shuffled_corpus.csv\" ) open ( shuffeled_file_path , 'w' ) . writelines ( lines ) # log metadata exp . log_metric ( \"lines\" , len ( lines )) # upload data env . upload_file ( shuffeled_file_path , \"dataset\" ) # Define parameter configuration for experiment run params = { 'seed' : 1 } # Run experiment and automatically sync all metadata exp . run_exp ( shuffle_lines , params ) # show experiment info # The same information are accesible from the experiment dashboard in Lab exp . print_info () More information about the lab-client library are available in the Lab Python Client section. Deploy Locally \u00b6 If you want to deploy the workspace stand-alone on your computer, just execute the following command: docker run -d -v /some/host/path:/workspace -p 8080 :8080 mltooling/ml-workspace:latest Most recent version: 1.1.2 The mount is there in case you want to persist all notebooks and files under /workspace on the host filesystem. If you want to connect to an ML Lab project, you can use our Python library also in this setup. Parameters \u00b6 To limit cpu and memory etc., you can pass Docker-specific flags to the container as stated in the Docker documentation . You can also pass following environment variables to configure things such as basic authentication etc.: Variable Description Default SERVICE_SSL_ENABLED Enable or disable SSL. When set to true, either certifcates must be mounted to SSL_RESOURCES_PATH or, if not, the container generates own certificates. false SSL_RESOURCES_PATH The container path where the SSL certficate files cert.key and cert.crt are mounted. /resources/ssl SERVICE_AUTH_ENABLED Enable or disable basic authentication. If enabled, also SERVICE_USER and SERVICE_PASSWORD should be set. false SERVICE_USER Basic auth user name. Only used when SERVICE_AUTH_ENABLED is true. user SERVICE_PASSWORD Basic auth password. Only used when SERVICE_AUTH_ENABLED is true. user WORKSPACE_BASE_URL The base URL under which the notebook server is reachable. E.g. setting it to /workspace, the workspace would be reachable under /workspace/tree / WORKSPACE_STORAGE_LIMIT The storage limit which will cause a pop-up to occur to remind the user to delete files. If not set, there is no limit NOTEBOOK_ARGS Additional flags that are passed to the jupyter notebook server on start separated by whitespace, e.g. --MappingKernelManager.cull_idle_timeout=172800. Has to take the form \"--MappingKernelManager.cull_idle_timeout=172800 --MappingKernelManager.cull_connected=True\" Integrated Tools \u00b6 In addition to the VNC access and Git Integrations, the workspace is equipped with a selection of common tools to help with the data science and experimentation workflow. Many of these tools can be started from the Open Tools menu. Desktop GUI \u00b6 The workspace provides an HTTP-based VNC access to the workspace via noVNC . Thereby, you can access and work within the workspace with a fully featured desktop GUI. To access this desktop GUI, go to Open Tools , select VNC , and click the Connect button. In the case you are asked for a password, use vncpassword . Once you are connected, you will see a desktop GUI that allows you to install and use advanced tools such as PyCharm, Visual Studio Code, Spyder and many more. Clipboard: If you want to share the clipboard between your computer and the workspace, you can use the copy-paste functionality as described below: Long-running tasks Use the desktop GUI for long-running Jupyter executions. By running notebooks from the browser of your workspace desktop GUI, all output will be synchronized to the notebook even if you have disconnected your computer from the notebook. Git Integration \u00b6 Version control is a crucial aspect in collaboration. To make this process as smooth as possible, we have integrated a Jupyter plugin specialized on pushing notebooks, a full-fledged web-based Git client (ungit), as well as a notebook merging tool (nbdime). Clone Repository \u00b6 To clone a Git repository, you can either use https with authentication via credentials or ssh with authentication via public key. For cloning repositories via https , we recommend to use Ungit ( Open Tool -> Ungit ) as shown below. If authentication is required, you will get asked for your credentials. If you prefer authentication via ssh public key, we recommend to use the clone functionality of the Jupyter Git plugin as shown below. If authentication is required, the plugin will generate an ssh key that you have to add to your account on the Git server. Once you have cloned the repository, it will appear at the selected location in your /workspace folder. Push, Pull, Merge and Other Git Actions \u00b6 To commit and push a single notebook to the remote Git repository, we recommend to use the Git plugin integrated into Jupyter as shown below: The other tool for more advanced Git integrations is ungit. It offers a clean and intuitive web-based UI that makes it convenient to sync your code artifacts. With ungit, you can do most of the common git actions such as push, pull, merge, branch, tag, checkout, and many more. Solve Merge Conflicts \u00b6 The workspace is pre-installed with nbdime , a tool for diffing and merging of Jupyter notebooks. Nbdime understands the structure of notebook documents and, therefore, can make intelligent decisions when diffing and merging notebooks. In the case you have merge conflicts, nbdime will make sure that the notebook is still readable by Jupyter. Netdata \u00b6 Netdata ( Open Tool -> Netdata ) is a real-time hardware and performance monitoring dashboard that visualise the processes and services on your Linux systems. It monitors metrics about CPU, GPU, memory, disks, networks, processes, and more. Use this tool during model training and other experimentation to get insights of everything happening on the system and figure out performance bottlenecks. Info Netdata will show you the hardware statistics of the server on which the workspace container is running. Glances \u00b6 Glances ( Open Tool -> Glances ) is a web-based hardware and performance monitoring dashboard. It is an an alternative to Netdata. JupyterLab \u00b6 JupyterLab ( Open Tool -> JupyterLab ) is the next-generation user interface for Project Jupyter. It offers all the familiar building blocks of the classic Jupyter Notebook (notebook, terminal, text editor, file browser, rich outputs, etc.) in a flexible and powerful user interface. This JupyterLab instance comes pre-installed with a few helpful extensions such as a the jupyterlab-toc , jupyterlab-git , and juptyterlab-tensorboard . Tensorboard \u00b6 Tensorboard provides a suite of visualization tools to make it easier to understand, debug, and optimize you experiment runs. It includes logging features for scalar, histogram, model structure, embeddings, and text & image visualization. We have integrated Tensorboard into the Jupyter interface with functionalities to start, manage, and stop instances. You can open a new instance for a valid logs directory as shown below: If you have opened a Tensorboard instance in a valid log directory, you will see the visualizations of your logged data. Experiment Tracking Integration The experiment tracking from the ML Lab client library has tensorboard logging capabilities integrated for any machine learning framework (see the Environment Tutorial).","title":"Workspace"},{"location":"usage/lab-workspace/#introduction-to-the-workspace","text":"The workspace is an all-in-one web IDE specialized for machine learning and data science. It comes with Jupyter Notebook, a Desktop GUI, Git Integration, Tensorboard Integration, Hardware Monitoring, and many common ML libraries. It is also project-independent, therefore, you will use the same workspace for all of the projects within the given ML Lab instance. The workspace you\u2019re using, including any custom files and libraries that you have setup, will not be accessible by any other user. The workspace is pre-installed with a variety of common data science libraries and features such as Tensorflow, Pytorch, Keras, Sklearn, XGboost, and many more.","title":"Introduction to the Workspace"},{"location":"usage/lab-workspace/#jupyter-basics","text":"Jupyter Notebook (the application you are currently using) is a web-based interactive environment for writing and running code. The main building blocks of Jupyter are the file-browser, the notebook editor, and kernels. The file-browser provides an interactive file manager for all notebooks, files, and folders in the /workspace directory. A new notebook can be created by clicking on the New drop-down button at the top of the list and selecting the desired language kernel. Terminal You can spawn interactive terminal instances as well by selecting New -> Terminal in the file-browser. The notebook editor enables users to author documents that include live code, markdown text, shell commands, LaTeX equations, interactive widgets, plots, and images. These notebook documents provide a complete and self-contained record of a computation that can be converted to various formats and shared with others. An interactive tour of the notebook UI can be started by selecting Help -> User Interface Tour from the notebook menu bar. If you are a first-time Jupyter user, you can find a more detailed introduction in the Jupyter Basics Tutorial inside your workspace as well. Extensions The workspace has a variety of third-party Jupyter extensions activated. You can configure these extensions in the nbextensions configurator: Edit -> nbextensions config The Notebook allows code to be run in a range of different programming languages. For each notebook document that a user opens, the web application starts a kernel that runs the code for that notebook and returns output. The workspace has a Python 3 and Python 2 kernel pre-installed. Additional Kernels can be installed to get access to other languages (R, Scala, Go, ...) or additional computing resources (GPUs, CPUs, Memory, ...). Python 2 Support Python 2 support is deprecated and not fully supported. Please only use Python 2 if necessary!","title":"Jupyter Basics"},{"location":"usage/lab-workspace/#workspace-folder","text":"The default work directory is /workspace which is also the root directory of the Jupyter instance. Everything in that directory is persisted on the underlying host and backed up to the configured ML Lab instance every day. Data Persistence Use the /workspace directory for all your important work artifacts! Data within other directories might get lost.","title":"/workspace Folder"},{"location":"usage/lab-workspace/#install-dependencies","text":"Within your workspace you have full root & sudo access to install any library or tool you need by using !pip install or !apt-get install inside the notebook, as shown below: !pip install matplotlib-venn Install Dependencies in Notebook It\u2019s a good idea to include cells which install and load any custom libraries or files (which are not pre-installed in the workspace) that your notebook needs.","title":"Install Dependencies"},{"location":"usage/lab-workspace/#connect-to-ml-lab","text":"The workspace has a python lab-client library pre-installed which provides easy access to the ML Lab API and capabilities to download/upload data, and run experiments. Below is a sneak peak into how you can use this library. To be able to execute this code, please replace LAB_PROJECT with one of your projects on the connected ML Lab instance and upload this dataset to the connected project from dataset section of the ML Lab UI. Info To run this code you need to have created a project and uploaded the provided dataset file to the ML Lab instance! from lab_client import Environment import random # Initialize environment env = Environment ( project = \"LAB_PROJECT\" , # ML Lab project you want to work on # Only required in stand-alone workspace deployments # lab_endpoint=\"LAB_ENDPOINT\", # lab_api_token=\"LAB_TOKEN\" ) # Show environment information env . print_info () # Create experiment exp = env . create_experiment ( 'Welcome Tutorial' ) # Get file (make sure that you have uploaded the dataset in the connected project) text_corpus_path = env . get_file ( 'datasets/news-categorized.csv' ) # Define experiment def shuffle_lines ( exp , params , artifacts ): # for this example we will just shuffle all lines in a file random . seed ( params [ \"seed\" ]) lines = open ( text_corpus_path ) . readlines () random . shuffle ( lines ) shuffeled_file_path = exp . create_file_path ( \"shuffled_corpus.csv\" ) open ( shuffeled_file_path , 'w' ) . writelines ( lines ) # log metadata exp . log_metric ( \"lines\" , len ( lines )) # upload data env . upload_file ( shuffeled_file_path , \"dataset\" ) # Define parameter configuration for experiment run params = { 'seed' : 1 } # Run experiment and automatically sync all metadata exp . run_exp ( shuffle_lines , params ) # show experiment info # The same information are accesible from the experiment dashboard in Lab exp . print_info () More information about the lab-client library are available in the Lab Python Client section.","title":"Connect to ML Lab"},{"location":"usage/lab-workspace/#deploy-locally","text":"If you want to deploy the workspace stand-alone on your computer, just execute the following command: docker run -d -v /some/host/path:/workspace -p 8080 :8080 mltooling/ml-workspace:latest Most recent version: 1.1.2 The mount is there in case you want to persist all notebooks and files under /workspace on the host filesystem. If you want to connect to an ML Lab project, you can use our Python library also in this setup.","title":"Deploy Locally"},{"location":"usage/lab-workspace/#parameters","text":"To limit cpu and memory etc., you can pass Docker-specific flags to the container as stated in the Docker documentation . You can also pass following environment variables to configure things such as basic authentication etc.: Variable Description Default SERVICE_SSL_ENABLED Enable or disable SSL. When set to true, either certifcates must be mounted to SSL_RESOURCES_PATH or, if not, the container generates own certificates. false SSL_RESOURCES_PATH The container path where the SSL certficate files cert.key and cert.crt are mounted. /resources/ssl SERVICE_AUTH_ENABLED Enable or disable basic authentication. If enabled, also SERVICE_USER and SERVICE_PASSWORD should be set. false SERVICE_USER Basic auth user name. Only used when SERVICE_AUTH_ENABLED is true. user SERVICE_PASSWORD Basic auth password. Only used when SERVICE_AUTH_ENABLED is true. user WORKSPACE_BASE_URL The base URL under which the notebook server is reachable. E.g. setting it to /workspace, the workspace would be reachable under /workspace/tree / WORKSPACE_STORAGE_LIMIT The storage limit which will cause a pop-up to occur to remind the user to delete files. If not set, there is no limit NOTEBOOK_ARGS Additional flags that are passed to the jupyter notebook server on start separated by whitespace, e.g. --MappingKernelManager.cull_idle_timeout=172800. Has to take the form \"--MappingKernelManager.cull_idle_timeout=172800 --MappingKernelManager.cull_connected=True\"","title":"Parameters"},{"location":"usage/lab-workspace/#integrated-tools","text":"In addition to the VNC access and Git Integrations, the workspace is equipped with a selection of common tools to help with the data science and experimentation workflow. Many of these tools can be started from the Open Tools menu.","title":"Integrated Tools"},{"location":"usage/lab-workspace/#desktop-gui","text":"The workspace provides an HTTP-based VNC access to the workspace via noVNC . Thereby, you can access and work within the workspace with a fully featured desktop GUI. To access this desktop GUI, go to Open Tools , select VNC , and click the Connect button. In the case you are asked for a password, use vncpassword . Once you are connected, you will see a desktop GUI that allows you to install and use advanced tools such as PyCharm, Visual Studio Code, Spyder and many more. Clipboard: If you want to share the clipboard between your computer and the workspace, you can use the copy-paste functionality as described below: Long-running tasks Use the desktop GUI for long-running Jupyter executions. By running notebooks from the browser of your workspace desktop GUI, all output will be synchronized to the notebook even if you have disconnected your computer from the notebook.","title":"Desktop GUI"},{"location":"usage/lab-workspace/#git-integration","text":"Version control is a crucial aspect in collaboration. To make this process as smooth as possible, we have integrated a Jupyter plugin specialized on pushing notebooks, a full-fledged web-based Git client (ungit), as well as a notebook merging tool (nbdime).","title":"Git Integration"},{"location":"usage/lab-workspace/#clone-repository","text":"To clone a Git repository, you can either use https with authentication via credentials or ssh with authentication via public key. For cloning repositories via https , we recommend to use Ungit ( Open Tool -> Ungit ) as shown below. If authentication is required, you will get asked for your credentials. If you prefer authentication via ssh public key, we recommend to use the clone functionality of the Jupyter Git plugin as shown below. If authentication is required, the plugin will generate an ssh key that you have to add to your account on the Git server. Once you have cloned the repository, it will appear at the selected location in your /workspace folder.","title":"Clone Repository"},{"location":"usage/lab-workspace/#push-pull-merge-and-other-git-actions","text":"To commit and push a single notebook to the remote Git repository, we recommend to use the Git plugin integrated into Jupyter as shown below: The other tool for more advanced Git integrations is ungit. It offers a clean and intuitive web-based UI that makes it convenient to sync your code artifacts. With ungit, you can do most of the common git actions such as push, pull, merge, branch, tag, checkout, and many more.","title":"Push, Pull, Merge and Other Git Actions"},{"location":"usage/lab-workspace/#solve-merge-conflicts","text":"The workspace is pre-installed with nbdime , a tool for diffing and merging of Jupyter notebooks. Nbdime understands the structure of notebook documents and, therefore, can make intelligent decisions when diffing and merging notebooks. In the case you have merge conflicts, nbdime will make sure that the notebook is still readable by Jupyter.","title":"Solve Merge Conflicts"},{"location":"usage/lab-workspace/#netdata","text":"Netdata ( Open Tool -> Netdata ) is a real-time hardware and performance monitoring dashboard that visualise the processes and services on your Linux systems. It monitors metrics about CPU, GPU, memory, disks, networks, processes, and more. Use this tool during model training and other experimentation to get insights of everything happening on the system and figure out performance bottlenecks. Info Netdata will show you the hardware statistics of the server on which the workspace container is running.","title":"Netdata"},{"location":"usage/lab-workspace/#glances","text":"Glances ( Open Tool -> Glances ) is a web-based hardware and performance monitoring dashboard. It is an an alternative to Netdata.","title":"Glances"},{"location":"usage/lab-workspace/#jupyterlab","text":"JupyterLab ( Open Tool -> JupyterLab ) is the next-generation user interface for Project Jupyter. It offers all the familiar building blocks of the classic Jupyter Notebook (notebook, terminal, text editor, file browser, rich outputs, etc.) in a flexible and powerful user interface. This JupyterLab instance comes pre-installed with a few helpful extensions such as a the jupyterlab-toc , jupyterlab-git , and juptyterlab-tensorboard .","title":"JupyterLab"},{"location":"usage/lab-workspace/#tensorboard","text":"Tensorboard provides a suite of visualization tools to make it easier to understand, debug, and optimize you experiment runs. It includes logging features for scalar, histogram, model structure, embeddings, and text & image visualization. We have integrated Tensorboard into the Jupyter interface with functionalities to start, manage, and stop instances. You can open a new instance for a valid logs directory as shown below: If you have opened a Tensorboard instance in a valid log directory, you will see the visualizations of your logged data. Experiment Tracking Integration The experiment tracking from the ML Lab client library has tensorboard logging capabilities integrated for any machine learning framework (see the Environment Tutorial).","title":"Tensorboard"},{"location":"walkthrough/lab-walkthrough/","text":"Walkthrough \u00b6 In the following visual walkthrough, you will experience most of the features, interaction patterns, and tools that the ML Lab offers. You can either just scroll through everything to get a quick overview of how our platform is meant to be used, or you can use this walkthrough as a tutorial by replicating it step-by-step. Let's get started! The first screen you will experience when visiting your ML Lab instance is this beautiful dashboard: This is your project's home section; it gives you an overview of the currently selected project and a section showing other available projects. If you are a new user, it will directly ask you to create your first project. We will get to this in the next section. Concept Definition: Project A project is a digital space for tackling a specific data science use-case. It consists of multiple datasets, experiments, models, services, and jobs. Create Project \u00b6 For this walkthrough, we will create a project which we will call demo-project . For your project you need to select a unique name that isn't already used by someone else on the ML Lab instance (e.g. <username>-project ). After the creation, the project will be selected and appear on you home section. Since it is a new project, it does not have any datasets, models, services, and experiments yet. Upload Data \u00b6 Concept Definition: Dataset A dataset is a single file that contains raw- or processed-data, usually preselected for a certain experiment. It is recommended to have the dataset in an easily readable format such as CSV for structured data or GraphML for graph data. If your dataset consists of multiple files (e.g. collection of images), we recommend zipping this dataset to a single file. Our python libraries have built-in support for his type of packaged data. To start the experimentation process, we first need to upload a dataset. Therefore, we switch to the datasets section and select the upload functionality: In this dialog, we can select and upload any file. For this walkthrough, we have prepared a dataset that contains a collection of news articles grouped into 20 categories ( 20 Newsgroups ). Our goal is to train a classification model that is able to predict the topic of a news article. You can download the dataset here . After we have uploaded the file, it will be shown as a dataset within the datasets table. Here we can see additional metadata and execute some actions, such as download, remove, and copy the datasets key to clipboard. Finally, to get started with experiments on this dataset, please navigate to the workspace section. Access your Workspace \u00b6 Concept Definition: Workspace A place for a data scientist to conduct a set of experiments centered on a given project or problem to solve. It can have many different notebooks and scripts containing different experiments that are carried out as the data scientist seek the best model. The workspace is implemented as an all-in-one web IDE specialized for machine learning and data science. If you are a first-time user, the workspace will need a few seconds to setup and get ready. After it is setup, you will see the Jupyter Notebook Web UI. This workspace is an all-in-one web IDE specialized for machine learning and data science. It comes with Jupyter Notebook, a Desktop GUI, Git Integration, Tensorboard Integration, Hardware Monitoring, and many common ML libraries. It is also project-independent , therefore, you will use the same workspace for all of the projects. The workspace is equipped with a selection of common tools to help with the data science and experimentation workflow. Many of these tools can be started from the Open Tool menu. Within the workspace you can use our python client library to connect to ML Lab, as we will show below. Experimentation \u00b6 Concept Definition: Experiment An experiment is a single execution of a data science code with specific parameters, data, and results. An experiment in the data science field usually refers to a single model training run, but can also be any other computational task, such as a data transformation, that requires a certain configuration and has some measurable results. All metadata and artifacts for an experiment such as parameters, timestamp, operator, data & model files, environment information, resulting metrics (e.g. accuracy), and other related information can easily be saved and shared with the experiment to enable reproducibility. During the experimentation process, one or more parameters are changed by the data scientist in an organized manner, and the effects of these changes on associated metrics are measured, recorded, and analyzed. Data scientist may try many different approaches, different parameters and fail many times before the required level of a metric is achieved. To get started with our experimentation, please open the text-classification.ipynb notebook inside the tutorials/ directory. Usually, we will have one notebook for every experiment. A notebook contains both computer code (e.g. python) and rich text elements (e.g. markdown). This notebook will guide us through the process of training a classification model for our dataset via sklearn . Tip In the notebook, you can execute any code cell with this shortcut: CTRL + Enter Clone Repository In this case, the notebook that contains the required code to train our classification model already exists in the workspace. However, sometimes you have to download code from external sources and especially if you collaborate with others, tools such as Git should be used to stay in sync. Luckily, Git is already installed within the workspace and can be accessed via the terminal. Conveniently, we also put a web-based UI called Ungit into the workspace. You can open it from the Open Tool menu. Initialize Environment \u00b6 Before we can get started with model training, we have to initialize the environment. The environment provides functionality to manage all project-related files (e.g. models & datasets) and experiments. The environment can be connected to an ML Lab instance by providing the ML Lab endpoint URL and a valid API token. If connected, the environment provides easy access to the ML Lab API and capabilities to download/upload data, and sync experiments. Locally, the environment has a dedicated folder structure to save and load datasets, models, and experiment data. Please insert the correct project-name for LAB_PROJECT . Within this code section, we also initialize an experiment instance with the create_experiment method for our notebook. An experiment should be used to organize all metadata and resources such as parameters, timestamps, input & output files, resulting metrics (e.g. accuracy), and other related information for a single experiment run such as the model training within our demo notebook. Download Data \u00b6 Once we have succesfully initialized the environment, we have full access to all the datasets, models, and services of the configured ML Lab project. A few minutes ago, we uploaded a dataset into our project. To access this dataset within or workspace, we need to get the unique file-key of the dataset. We can copy this key in the datasets page of our ML Lab instance by just clicking on the row of the choosen dataset. Once we have the unique key for the dataset, we can use the get_file method in our experiment notebook to download the file into our workspace: Next, please run the explore and transform data sections to prepare the data for model training. Run Experiment \u00b6 Finally, we arrived at the section where the magic happens. First we define our training algorithm and wrap it within a train function. Next, we need to define our hyperparameter configuration for this experiment run (in the params dict) and execute the run_exp function from the Experiment instance with the implemented train function and the params dictionary. After the execution the training code, the experiment will require a short time for training. During this training, we offer some tools that helps us to monitor our experiment (accesible from the Open Tools selection). For example, you can use netdata during model training and other experimentation to get insights of everything happening on the system (CPU, GPU, memory, disks...) and figure out performance bottlenecks. ML Lab also has integrated an experimentation dashboard for every project, that provides an overview for the team on all running and finished experiments with a lot of metadata to support reproducibility and transparency. Create Unified Model \u00b6 Concept Definition: Model A model is an artifact that is created in the training process. A model is something that predicts an output for a given input. Any file created after training from an ML algorithm is a model. The model needs to be deployed as a service in order to offer the model's prediction capabilities for integration into applications. Once the training is finished, we will get a trained model instance as result which we can now use to predict an output for any given input. This model needs to be deployed as a service in order to offer the model's prediction capabilities for integration into applications. In order to easily share and deploy the trained model, we will use the unified model library to create a self-contained executable model file. Upload Model \u00b6 After a final evaluation of the model accuracy, we want to make the trained model available for deployment so that it can be used from any application. Therefore, we need to upload the trained model artifact into the model storage of our ML Lab project from within the classification notebook via the upload_file method: Data Versioning Our dataset and model store supports file versioning. This prevents you from overwriting any data or models. Once we have successfully uploaded the model, we are finished with the notebook for this walktrough. Deploy Model \u00b6 Concept Definition: Service A service is a software component that implements and provides specific capabilities. In our landscape, services are deployed as Docker containers and are usually accessible via a REST API over HTTP(S). In order to deploy the uploaded model as a service into production, navigate to the Services page in the ML Lab. Click on the \"Add Service\" button and fill in the required fields. There are multiple possibilities how you provide the needed information. Either you create a Docker image that already contains the model or you create a generic image and provide information of how to download the model upon container start via an environment variable. For the latter one we recommend to use the ML Lab Python library in the image and just pass the model's name as an environment variable. ML Lab will inject other needed information such as project name, access token, and URL as environment variables automatically which are used by the Python library. One image that can be used for such a generic use-case is the Unified Model . You can copy the model's path in the Models section of the webapp. After a few seconds, the model service will appear on the services page of the ML Lab. If the deployed image follows the ML Lab service specification, you can access the REST API documentation by clicking the access button on the created model service and try the predict method with any kind of input data. Congratulations we have successfully trained a text classification model for the uploaded dataset and deployed it into production. Next Steps \u00b6 Workspace : Explore your workspace. Python Client : Learn more on how to use the python client libary.","title":"ML Lab Walkthrough"},{"location":"walkthrough/lab-walkthrough/#walkthrough","text":"In the following visual walkthrough, you will experience most of the features, interaction patterns, and tools that the ML Lab offers. You can either just scroll through everything to get a quick overview of how our platform is meant to be used, or you can use this walkthrough as a tutorial by replicating it step-by-step. Let's get started! The first screen you will experience when visiting your ML Lab instance is this beautiful dashboard: This is your project's home section; it gives you an overview of the currently selected project and a section showing other available projects. If you are a new user, it will directly ask you to create your first project. We will get to this in the next section. Concept Definition: Project A project is a digital space for tackling a specific data science use-case. It consists of multiple datasets, experiments, models, services, and jobs.","title":"Walkthrough"},{"location":"walkthrough/lab-walkthrough/#create-project","text":"For this walkthrough, we will create a project which we will call demo-project . For your project you need to select a unique name that isn't already used by someone else on the ML Lab instance (e.g. <username>-project ). After the creation, the project will be selected and appear on you home section. Since it is a new project, it does not have any datasets, models, services, and experiments yet.","title":"Create Project"},{"location":"walkthrough/lab-walkthrough/#upload-data","text":"Concept Definition: Dataset A dataset is a single file that contains raw- or processed-data, usually preselected for a certain experiment. It is recommended to have the dataset in an easily readable format such as CSV for structured data or GraphML for graph data. If your dataset consists of multiple files (e.g. collection of images), we recommend zipping this dataset to a single file. Our python libraries have built-in support for his type of packaged data. To start the experimentation process, we first need to upload a dataset. Therefore, we switch to the datasets section and select the upload functionality: In this dialog, we can select and upload any file. For this walkthrough, we have prepared a dataset that contains a collection of news articles grouped into 20 categories ( 20 Newsgroups ). Our goal is to train a classification model that is able to predict the topic of a news article. You can download the dataset here . After we have uploaded the file, it will be shown as a dataset within the datasets table. Here we can see additional metadata and execute some actions, such as download, remove, and copy the datasets key to clipboard. Finally, to get started with experiments on this dataset, please navigate to the workspace section.","title":"Upload Data"},{"location":"walkthrough/lab-walkthrough/#access-your-workspace","text":"Concept Definition: Workspace A place for a data scientist to conduct a set of experiments centered on a given project or problem to solve. It can have many different notebooks and scripts containing different experiments that are carried out as the data scientist seek the best model. The workspace is implemented as an all-in-one web IDE specialized for machine learning and data science. If you are a first-time user, the workspace will need a few seconds to setup and get ready. After it is setup, you will see the Jupyter Notebook Web UI. This workspace is an all-in-one web IDE specialized for machine learning and data science. It comes with Jupyter Notebook, a Desktop GUI, Git Integration, Tensorboard Integration, Hardware Monitoring, and many common ML libraries. It is also project-independent , therefore, you will use the same workspace for all of the projects. The workspace is equipped with a selection of common tools to help with the data science and experimentation workflow. Many of these tools can be started from the Open Tool menu. Within the workspace you can use our python client library to connect to ML Lab, as we will show below.","title":"Access your Workspace"},{"location":"walkthrough/lab-walkthrough/#experimentation","text":"Concept Definition: Experiment An experiment is a single execution of a data science code with specific parameters, data, and results. An experiment in the data science field usually refers to a single model training run, but can also be any other computational task, such as a data transformation, that requires a certain configuration and has some measurable results. All metadata and artifacts for an experiment such as parameters, timestamp, operator, data & model files, environment information, resulting metrics (e.g. accuracy), and other related information can easily be saved and shared with the experiment to enable reproducibility. During the experimentation process, one or more parameters are changed by the data scientist in an organized manner, and the effects of these changes on associated metrics are measured, recorded, and analyzed. Data scientist may try many different approaches, different parameters and fail many times before the required level of a metric is achieved. To get started with our experimentation, please open the text-classification.ipynb notebook inside the tutorials/ directory. Usually, we will have one notebook for every experiment. A notebook contains both computer code (e.g. python) and rich text elements (e.g. markdown). This notebook will guide us through the process of training a classification model for our dataset via sklearn . Tip In the notebook, you can execute any code cell with this shortcut: CTRL + Enter Clone Repository In this case, the notebook that contains the required code to train our classification model already exists in the workspace. However, sometimes you have to download code from external sources and especially if you collaborate with others, tools such as Git should be used to stay in sync. Luckily, Git is already installed within the workspace and can be accessed via the terminal. Conveniently, we also put a web-based UI called Ungit into the workspace. You can open it from the Open Tool menu.","title":"Experimentation"},{"location":"walkthrough/lab-walkthrough/#initialize-environment","text":"Before we can get started with model training, we have to initialize the environment. The environment provides functionality to manage all project-related files (e.g. models & datasets) and experiments. The environment can be connected to an ML Lab instance by providing the ML Lab endpoint URL and a valid API token. If connected, the environment provides easy access to the ML Lab API and capabilities to download/upload data, and sync experiments. Locally, the environment has a dedicated folder structure to save and load datasets, models, and experiment data. Please insert the correct project-name for LAB_PROJECT . Within this code section, we also initialize an experiment instance with the create_experiment method for our notebook. An experiment should be used to organize all metadata and resources such as parameters, timestamps, input & output files, resulting metrics (e.g. accuracy), and other related information for a single experiment run such as the model training within our demo notebook.","title":"Initialize Environment"},{"location":"walkthrough/lab-walkthrough/#download-data","text":"Once we have succesfully initialized the environment, we have full access to all the datasets, models, and services of the configured ML Lab project. A few minutes ago, we uploaded a dataset into our project. To access this dataset within or workspace, we need to get the unique file-key of the dataset. We can copy this key in the datasets page of our ML Lab instance by just clicking on the row of the choosen dataset. Once we have the unique key for the dataset, we can use the get_file method in our experiment notebook to download the file into our workspace: Next, please run the explore and transform data sections to prepare the data for model training.","title":"Download Data"},{"location":"walkthrough/lab-walkthrough/#run-experiment","text":"Finally, we arrived at the section where the magic happens. First we define our training algorithm and wrap it within a train function. Next, we need to define our hyperparameter configuration for this experiment run (in the params dict) and execute the run_exp function from the Experiment instance with the implemented train function and the params dictionary. After the execution the training code, the experiment will require a short time for training. During this training, we offer some tools that helps us to monitor our experiment (accesible from the Open Tools selection). For example, you can use netdata during model training and other experimentation to get insights of everything happening on the system (CPU, GPU, memory, disks...) and figure out performance bottlenecks. ML Lab also has integrated an experimentation dashboard for every project, that provides an overview for the team on all running and finished experiments with a lot of metadata to support reproducibility and transparency.","title":"Run Experiment"},{"location":"walkthrough/lab-walkthrough/#create-unified-model","text":"Concept Definition: Model A model is an artifact that is created in the training process. A model is something that predicts an output for a given input. Any file created after training from an ML algorithm is a model. The model needs to be deployed as a service in order to offer the model's prediction capabilities for integration into applications. Once the training is finished, we will get a trained model instance as result which we can now use to predict an output for any given input. This model needs to be deployed as a service in order to offer the model's prediction capabilities for integration into applications. In order to easily share and deploy the trained model, we will use the unified model library to create a self-contained executable model file.","title":"Create Unified Model"},{"location":"walkthrough/lab-walkthrough/#upload-model","text":"After a final evaluation of the model accuracy, we want to make the trained model available for deployment so that it can be used from any application. Therefore, we need to upload the trained model artifact into the model storage of our ML Lab project from within the classification notebook via the upload_file method: Data Versioning Our dataset and model store supports file versioning. This prevents you from overwriting any data or models. Once we have successfully uploaded the model, we are finished with the notebook for this walktrough.","title":"Upload Model"},{"location":"walkthrough/lab-walkthrough/#deploy-model","text":"Concept Definition: Service A service is a software component that implements and provides specific capabilities. In our landscape, services are deployed as Docker containers and are usually accessible via a REST API over HTTP(S). In order to deploy the uploaded model as a service into production, navigate to the Services page in the ML Lab. Click on the \"Add Service\" button and fill in the required fields. There are multiple possibilities how you provide the needed information. Either you create a Docker image that already contains the model or you create a generic image and provide information of how to download the model upon container start via an environment variable. For the latter one we recommend to use the ML Lab Python library in the image and just pass the model's name as an environment variable. ML Lab will inject other needed information such as project name, access token, and URL as environment variables automatically which are used by the Python library. One image that can be used for such a generic use-case is the Unified Model . You can copy the model's path in the Models section of the webapp. After a few seconds, the model service will appear on the services page of the ML Lab. If the deployed image follows the ML Lab service specification, you can access the REST API documentation by clicking the access button on the created model service and try the predict method with any kind of input data. Congratulations we have successfully trained a text classification model for the uploaded dataset and deployed it into production.","title":"Deploy Model"},{"location":"walkthrough/lab-walkthrough/#next-steps","text":"Workspace : Explore your workspace. Python Client : Learn more on how to use the python client libary.","title":"Next Steps"}]}